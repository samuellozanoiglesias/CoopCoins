{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX is using: TFRT_CPU_0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"cpu\"  # Forces JAX to run on CPU\n",
    "\n",
    "import jax #pip install jax\n",
    "print(f\"JAX is using: {jax.devices()[0]}\")\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax #pip install distrax\n",
    "from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper\n",
    "import jaxmarl\n",
    "from jaxmarl.wrappers.baselines import LogWrapper\n",
    "from jaxmarl.environments.overcooked import Overcooked, overcooked_layouts, layout_grid_to_dict\n",
    "from jaxmarl.viz.overcooked_visualizer import OvercookedVisualizer\n",
    "from jaxmarl import make\n",
    "import hydra #pip install hydra-core\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Added to modify the structure of the definitions\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.lax as lax\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import time\n",
    "import re\n",
    "from textwrap import dedent\n",
    "\n",
    "# Save the model parameters only\n",
    "def save_model(runner_state, save_dir, file):\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Construct the full file path\n",
    "    filename = os.path.join(save_dir, file)\n",
    "\n",
    "    # Extract train_state (first element of the tuple)\n",
    "    train_state = runner_state[0]\n",
    "\n",
    "    # Save only the model parameters\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump({'params': train_state.params}, f)\n",
    "\n",
    "    print(f\"Model parameters saved to {filename}\")\n",
    "\n",
    "# Load the model parameters\n",
    "def load_model(load_dir, file, network):\n",
    "    filename = os.path.join(load_dir, file)\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f\"Model file {filename} not found.\")\n",
    "\n",
    "    with open(filename, \"rb\") as f:\n",
    "        saved_data = pickle.load(f)\n",
    "\n",
    "    # Ensure the loaded model has the correct parameter structure\n",
    "    if \"params\" not in saved_data:\n",
    "        raise ValueError(\"Invalid saved model file: missing 'params'.\")\n",
    "\n",
    "    model_params = saved_data[\"params\"]\n",
    "    print(f\"Model parameters loaded from {filename}\")\n",
    "\n",
    "    return model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_tx(config):\n",
    "    def linear_schedule(count):\n",
    "        frac = 1.0 - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"])) / config[\"NUM_UPDATES\"]\n",
    "        return config[\"LR\"] * frac\n",
    "    \n",
    "    if config[\"ANNEAL_LR\"]:\n",
    "        tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "        )\n",
    "    else:\n",
    "        tx = optax.chain(optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]), optax.adam(config[\"LR\"], eps=1e-5))\n",
    "    \n",
    "    return tx\n",
    "\n",
    "def find_closest_checkpoint(fixed_step, load_dir):\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(load_dir)\n",
    "    \n",
    "    # Filter files that match the pattern 'trained_model_{step}.pkl'\n",
    "    checkpoint_files = [f for f in files if f.startswith(\"trained_model_\") and f.endswith(\".pkl\")]\n",
    "    \n",
    "    # Extract the step numbers from the filenames\n",
    "    steps = []\n",
    "    for file in checkpoint_files:\n",
    "        step_str = file.split('_')[-1].split('.')[0]  # Extract the step number\n",
    "        try:\n",
    "            step = int(step_str)\n",
    "            steps.append(step)\n",
    "        except ValueError:\n",
    "            continue  # Skip files that don't have a valid step number\n",
    "    \n",
    "    if len(steps) == 0:\n",
    "        raise FileNotFoundError(f\"No checkpoint files found in {load_dir}\")\n",
    "    \n",
    "    # Find the closest step to the fixed step\n",
    "    closest_step = min(steps, key=lambda x: abs(x - fixed_step))\n",
    "    closest_filename = f\"trained_model_{closest_step}.pkl\"\n",
    "    \n",
    "    print(f\"Found checkpoint: {closest_filename} for step {closest_step}\")\n",
    "    \n",
    "    # Load the model (assuming load_model function is available)\n",
    "    loaded_params = load_model(load_dir, closest_filename, network)\n",
    "    \n",
    "    return loaded_params, closest_step\n",
    "\n",
    "import imageio.v3 as iio\n",
    "\n",
    "def custom_animate(state_seq, agent_view_size, filename=\"animation.gif\"):\n",
    "    \"\"\"Animate a GIF give a state sequence and save if to file.\"\"\"\n",
    "    import imageio\n",
    "    TILE_PIXELS = 32\n",
    "\n",
    "    padding = agent_view_size - 2  # show\n",
    "\n",
    "    def get_frame(state):\n",
    "            grid = np.asarray(state.maze_map[padding:-padding, padding:-padding, :])\n",
    "            # Render the state\n",
    "            frame = OvercookedVisualizer._render_grid(\n",
    "                    grid,\n",
    "                    tile_size=TILE_PIXELS,\n",
    "                    highlight_mask=None,\n",
    "                    agent_dir_idx=state.agent_dir_idx,\n",
    "                    agent_inv=state.agent_inv\n",
    "            )\n",
    "            return frame\n",
    "\n",
    "    frame_seq =[get_frame(state) for state in state_seq]\n",
    "    \n",
    "    imageio.mimsave(filename, frame_seq, 'GIF', duration=2)\n",
    "    \n",
    "def gif_to_mp4(gif_filename):\n",
    "    \"\"\"Convert a GIF to MP4 using imageio and ffmpeg.\"\"\"\n",
    "    mp4_filename = gif_filename.replace(\".gif\", \".mp4\")\n",
    "    gif = iio.imread(gif_filename)\n",
    "    fps = 2  # Set desired FPS\n",
    "\n",
    "    iio.imwrite(mp4_filename, gif, extension=\".mp4\", fps=fps)\n",
    "    print(f\"MP4 saved to {mp4_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        pi = distrax.Categorical(logits=actor_mean)\n",
    "\n",
    "        critic = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "\n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_info(info):\n",
    "    def reshape_fn(x):\n",
    "        if isinstance(x, dict):\n",
    "            return jax.tree.map(lambda y: y.reshape(-1), x)\n",
    "        # For (16, 2) shaped arrays, reshape to (32,)\n",
    "        elif len(x.shape) == 2 and x.shape[1] == 2:\n",
    "            return x.reshape(-1)\n",
    "        # For (16,) shaped arrays\n",
    "        else:\n",
    "            # Repeat each element twice since we have 2 agents\n",
    "            return jnp.repeat(x, 2)\n",
    "    return jax.tree.map(reshape_fn, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(x: dict, agent_list, num_actors):\n",
    "    x = jnp.stack([x[a] for a in agent_list])\n",
    "    return x.reshape((num_actors, -1))\n",
    "\n",
    "def unbatchify(x: jnp.ndarray, agent_list, num_envs, num_actors):\n",
    "    x = x.reshape((num_actors, num_envs, -1))\n",
    "    return {a: x[i] for i, a in enumerate(agent_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_config(config):\n",
    "    env = jaxmarl.make(config[\"ENV_NAME\"], **config[\"ENV_KWARGS\"])\n",
    "    print(f'LR: {config[\"LR\"]}')\n",
    "    print(f'NUM_ENVS: {config[\"NUM_ENVS\"]}')\n",
    "    print(f'NUM_STEPS: {config[\"NUM_STEPS\"]}')\n",
    "    print(f'TOTAL_TIMESTEPS: {config[\"TOTAL_TIMESTEPS\"]}')\n",
    "    print(f'UPDATE_EPOCHS: {config[\"UPDATE_EPOCHS\"]}')\n",
    "    print(f'NUM_MINIBATCHES: {config[\"NUM_MINIBATCHES\"]}')\n",
    "\n",
    "    config[\"NUM_ACTORS\"] = env.num_agents * config[\"NUM_ENVS\"]\n",
    "    print(f'NUM_ACTORS: {config[\"NUM_ACTORS\"]}')\n",
    "\n",
    "    config[\"NUM_UPDATES\"] = int(\n",
    "        config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    print(f'NUM_UPDATES: {config[\"NUM_UPDATES\"]}')\n",
    "\n",
    "    config[\"NUM_SAVES\"] = int(config[\"NUM_UPDATES\"] // config[\"SAVE_EVERY_N_EPOCHS\"])\n",
    "    print(f'NUM_SAVES: {config[\"NUM_SAVES\"]}')\n",
    "\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ACTORS\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "    print(f'MINIBATCH_SIZE: {config[\"MINIBATCH_SIZE\"]}')\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train(config):\n",
    "    env = jaxmarl.make(config[\"ENV_NAME\"], **config[\"ENV_KWARGS\"])\n",
    "    \n",
    "    env = LogWrapper(env)\n",
    "    \n",
    "    def linear_schedule(count):\n",
    "        frac = 1.0 - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"])) / config[\"NUM_UPDATES\"]\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(seed, rng, save_dir):\n",
    "\n",
    "        # INIT NETWORK\n",
    "\n",
    "        # Creates an instance of the ActorCritic model.\n",
    "        # The ActorCritic class is initialized with:\n",
    "        # env.action_space().n: The number of possible actions in the environment.\n",
    "        # config[\"ACTIVATION\"]: The type of activation function to use (e.g., ReLU or Tanh).\n",
    "        network = ActorCritic(env.action_space().n, activation=config[\"ACTIVATION\"])\n",
    "\n",
    "        # Splits the random number generator rng into two separate RNGs (rng and _rng), so they can be used independently.\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "\n",
    "        # Creates a zero-initialized array of the same shape as the observation space of the environment (env.observation_space().shape), then flattens it into a 1D array (init_x).\n",
    "        # This serves as a sample input to initialize the network.\n",
    "        init_x = jnp.zeros(env.observation_space().shape)\n",
    "        init_x = init_x.flatten()\n",
    "\n",
    "        # Initializes the parameters of the network (network_params) by passing the random key _rng and the input init_x (observation example).\n",
    "        # This is necessary to set up the weights and biases of the neural network layers.\n",
    "        network_params = network.init(_rng, init_x)\n",
    "\n",
    "        # This block initializes the optimizer (tx) that will be used to update the network parameters.\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]), optax.adam(config[\"LR\"], eps=1e-5))\n",
    "\n",
    "        # Creates a TrainState object that holds the model's parameters (network_params), the optimizer (tx), and the function to apply the model (network.apply).\n",
    "        # This will be used for model training, including parameter updates.\n",
    "        train_state = TrainState.create(\n",
    "            apply_fn=network.apply,\n",
    "            params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        # INIT ENV\n",
    "        # Splits the RNG (rng) into two new RNGs: one (_rng) used for resetting the environment, and another (reset_rng) for each environment if you're running multiple environments in parallel (config[\"NUM_ENVS\"]).\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "\n",
    "        # Initializes the environment(s). jax.vmap is used to vectorize the env.reset function, so it can reset config[\"NUM_ENVS\"] environments in parallel.\n",
    "        # reset_rng: The random keys for resetting each environment are passed in here.\n",
    "        # obsv: The initial observations returned by the environment(s).\n",
    "        # env_state: The initial state of the environment(s).\n",
    "        obsv, env_state = jax.vmap(env.reset, in_axes=(0,))(reset_rng)\n",
    "        \n",
    "        # TRAIN LOOP\n",
    "        @jax.jit\n",
    "        def _update_step(runner_state, unused):\n",
    "            # COLLECT TRAJECTORIES\n",
    "\n",
    "            # Action Selection: It uses the current policy (pi) to sample an action based on the current observations.\n",
    "            # Environment Step: It then steps the environment using the selected actions, receiving observations, rewards, done flags, and additional info.\n",
    "            # Transition Recording: A transition is created, which includes the action taken, value estimate, rewards, and log probabilities for the action. This transition is used to calculate losses later.\n",
    "\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "\n",
    "                obs_batch = batchify(last_obs, env.agents, config[\"NUM_ACTORS\"])\n",
    "\n",
    "                pi, value = network.apply(train_state.params, obs_batch)\n",
    "                action = pi.sample(seed=_rng)\n",
    "                log_prob = pi.log_prob(action)\n",
    "                env_act = unbatchify(action, env.agents, config[\"NUM_ENVS\"], env.num_agents)\n",
    "\n",
    "                env_act = {k:v.flatten() for k,v in env_act.items()}\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "\n",
    "                obsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0,0,0))(\n",
    "                    rng_step, env_state, env_act\n",
    "                )\n",
    "\n",
    "                info = reshape_info(info)\n",
    "                transition = Transition(\n",
    "                    batchify(done, env.agents, config[\"NUM_ACTORS\"]).squeeze(),\n",
    "                    action,\n",
    "                    value,\n",
    "                    batchify(reward, env.agents, config[\"NUM_ACTORS\"]).squeeze(),\n",
    "                    log_prob,\n",
    "                    obs_batch,\n",
    "                    info\n",
    "\n",
    "                )\n",
    "                runner_state = (train_state, env_state, obsv, rng)\n",
    "                return runner_state, transition\n",
    "            \n",
    "            # This line performs the _env_step function repeatedly for NUM_STEPS. jax.lax.scan is a JAX function that allows you to apply a function repeatedly over a sequence of data, which is used here to simulate multiple environment interactions (collecting trajectories for NUM_STEPS).\n",
    "            runner_state, traj_batch = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "\n",
    "            # CALCULATE ADVANTAGE\n",
    "            # After collecting the trajectory, this part calculates the advantages of each transition using GAE. The last observation (last_obs) is passed through the network to get the value (last_val) for the last state.\n",
    "            train_state, env_state, last_obs, rng = runner_state\n",
    "            last_obs_batch = batchify(last_obs, env.agents, config[\"NUM_ACTORS\"])\n",
    "            _, last_val = network.apply(train_state.params, last_obs_batch)\n",
    "            \n",
    "            # This function implements the GAE algorithm to compute advantages and targets (the value targets).\n",
    "            # delta is computed using the Bellman equation for each transition.\n",
    "            # gae is calculated iteratively in reverse order of the trajectory, allowing for more stable learning by incorporating multiple future rewards.\n",
    "            def _calculate_gae(traj_batch, last_val):\n",
    "                def _get_advantages(gae_and_next_value, transition):\n",
    "                    gae, next_value = gae_and_next_value\n",
    "                    done, value, reward = (\n",
    "                        transition.done,\n",
    "                        transition.value,\n",
    "                        transition.reward,\n",
    "                    )\n",
    "                    delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
    "                    gae = (\n",
    "                        delta\n",
    "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "                    )\n",
    "                    return (gae, value), gae\n",
    "\n",
    "                _, advantages = jax.lax.scan(\n",
    "                    _get_advantages,\n",
    "                    (jnp.zeros_like(last_val), last_val),\n",
    "                    traj_batch,\n",
    "                    reverse=True,\n",
    "                    unroll=16,\n",
    "                )\n",
    "                return advantages, advantages + traj_batch.value\n",
    "\n",
    "            advantages, targets = _calculate_gae(traj_batch, last_val)\n",
    "\n",
    "            # UPDATE NETWORK\n",
    "            def _update_epoch(update_state, unused):\n",
    "\n",
    "                # This function updates the model by applying the computed gradients and losses.\n",
    "                # Loss Calculation: The loss function has two parts:\n",
    "                # Value loss: The loss for the value function is the mean squared error between predicted values and the target values (calculated using GAE).\n",
    "                # Actor loss: The loss for the policy (actor) is based on the surrogate objective from the PPO (Proximal Policy Optimization) algorithm, involving the ratio between the current and previous probabilities of actions.\n",
    "                # Entropy loss: A term to encourage exploration by adding entropy to the objective function.\n",
    "                def _update_minbatch(train_state, batch_info):\n",
    "                    traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                    def _loss_fn(params, traj_batch, gae, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        pi, value = network.apply(params, traj_batch.obs)\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        value_pred_clipped = traj_batch.value + (\n",
    "                            value - traj_batch.value\n",
    "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = (\n",
    "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "                        )\n",
    "\n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                        loss_actor1 = ratio * gae\n",
    "                        loss_actor2 = (\n",
    "                            jnp.clip(\n",
    "                                ratio,\n",
    "                                1.0 - config[\"CLIP_EPS\"],\n",
    "                                1.0 + config[\"CLIP_EPS\"],\n",
    "                            )\n",
    "                            * gae\n",
    "                        )\n",
    "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                        loss_actor = loss_actor.mean()\n",
    "                        entropy = pi.entropy().mean()\n",
    "\n",
    "                        total_loss = (\n",
    "                            loss_actor\n",
    "                            + config[\"VF_COEF\"] * value_loss\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                    total_loss, grads = grad_fn(\n",
    "                        train_state.params, traj_batch, advantages, targets\n",
    "                    )\n",
    "                    train_state = train_state.apply_gradients(grads=grads)\n",
    "                    return train_state, total_loss\n",
    "\n",
    "                # This code shuffles the data (traj_batch, advantages, targets) into minibatches for efficient training. The training data is reshaped, and the order is randomized to reduce bias during training.\n",
    "                train_state, traj_batch, advantages, targets, rng = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
    "                assert (\n",
    "                    batch_size == config[\"NUM_STEPS\"] * config[\"NUM_ACTORS\"]\n",
    "                ), \"batch size must be equal to number of steps * number of actors\"\n",
    "                permutation = jax.random.permutation(_rng, batch_size)\n",
    "                batch = (traj_batch, advantages, targets)\n",
    "                batch = jax.tree_util.tree_map(\n",
    "                    lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
    "                )\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "                )\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.reshape(\n",
    "                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "\n",
    "                # This line performs the minibatch updates using the scan function to apply the _update_minbatch function across all minibatches.\n",
    "                train_state, total_loss = jax.lax.scan(\n",
    "                    _update_minbatch, train_state, minibatches\n",
    "                )\n",
    "                update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "                return update_state, total_loss\n",
    "\n",
    "            # This line performs the epoch updates using the scan function to apply the _update_epoch function across all epochs.\n",
    "            update_state = (train_state, traj_batch, advantages, targets, rng)\n",
    "            update_state, loss_info = jax.lax.scan(\n",
    "                _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "\n",
    "            # After all epochs of training are completed, the train_state (the updated model) is returned along with metrics (e.g., information about rewards, losses, etc.), and the random number generator (rng) is updated.\n",
    "            train_state = update_state[0]\n",
    "            metric = traj_batch.info\n",
    "            rng = update_state[-1]\n",
    "\n",
    "            runner_state = (train_state, env_state, last_obs, rng)\n",
    "            return runner_state, metric\n",
    "\n",
    "        # This line splits the rng (random number generator) into two separate random number generators: rng and _rng. This is done so that each part of the code can use a different random stream. \n",
    "        rng, _rng = jax.random.split(rng)\n",
    "\n",
    "        # Here, the runner_state is initialized as a tuple that includes:\n",
    "        # train_state: The current state of the model (including parameters and optimization state).\n",
    "        # env_state: The current state of the environment (e.g., positions, internal states of agents).\n",
    "        # obsv: The current batch of observations (e.g., the states that the agents are observing from the environment).\n",
    "        # _rng: The random number generator that will be used for further random operations in the loop.\n",
    "        runner_state = (train_state, env_state, obsv, _rng)\n",
    "\n",
    "        # This line performs an update loop using jax.lax.scan. Here’s how it works:\n",
    "        # jax.lax.scan is a JAX function that allows you to loop over some operation while maintaining the state between iterations, making it ideal for iterative processes like training loops.\n",
    "        # The _update_step function is applied iteratively, where each iteration updates the state of the runner (i.e., the model and environment) and records the metrics (e.g., loss, performance metrics).\n",
    "        # runner_state: This contains all the information required for each update step (including train_state, env_state, etc.).\n",
    "        # None: The second argument is None because the loop doesn’t need any additional data passed each time; it's just evolving the state.\n",
    "        # config[\"NUM_UPDATES\"]: This specifies how many times the _update_step function will be applied. Essentially, this determines how many updates (iterations) will be made to the model.\n",
    "        #runner_state, metric = jax.lax.scan(\n",
    "        #    _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "        #)\n",
    "\n",
    "        metrics = {}\n",
    "        save_intervals = max(1, config[\"NUM_UPDATES\"] // config[\"NUM_SAVES\"])\n",
    "\n",
    "        for step in range(config[\"NUM_UPDATES\"]):\n",
    "            runner_state, metric = _update_step(runner_state, step)\n",
    "\n",
    "            if step % save_intervals == 0 or step == config[\"NUM_UPDATES\"] - 1:\n",
    "                save_model(runner_state, save_dir, f\"trained_model_{step}.pkl\")\n",
    "\n",
    "            # Store each metric separately in the dictionary\n",
    "            if not metrics:  # Initialize keys on the first iteration\n",
    "                metrics = {key: [] for key in metric}\n",
    "\n",
    "            for key in metric:\n",
    "                metrics[key].append(metric[key])  # Append new values for each key\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f\"Seed {seed}, step {step} completed\")\n",
    "\n",
    "        # After executing the loop, runner_state and metric will be updated:\n",
    "        # runner_state: The final state of the model, environment, and random number generator after all the updates.\n",
    "        # metric: A collection of metrics generated during the updates (e.g., losses, rewards, or performance indicators).\n",
    "        return {\"runner_state\": runner_state, \"metrics\": jax.tree.map(lambda x: jnp.stack(x), metrics)}\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one of the classic layouts (cramped_room, asymm_advantages, coord_ring, forced_coord, counter_circuit)\n",
    "# Or make your own! (P=pot, O=onion, A=agent, B=plates, X=deliver)\n",
    "\n",
    "def load_custom_layout(layout_name):\n",
    "    if layout_name == \"custom_1\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WWBWW\n",
    "        WA AW\n",
    "        P   P\n",
    "        W   W\n",
    "        WOXOW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "\n",
    "    elif layout_name == \"custom_2\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WWBWW\n",
    "        WA AW\n",
    "        P   W\n",
    "        P   W\n",
    "        WOXOW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "\n",
    "    elif layout_name == \"custom_3\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WBWWBW\n",
    "        P    P\n",
    "        W AA W\n",
    "        O    O\n",
    "        WXWWXW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "\n",
    "    elif layout_name == \"custom_4\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WBWXW\n",
    "        P W O\n",
    "        WAWAW\n",
    "        P W O\n",
    "        WBWXW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "\n",
    "    elif layout_name == \"custom_5\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WPWPW\n",
    "        W B W\n",
    "        WAWAW\n",
    "        W O W\n",
    "        WXWXW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "    \n",
    "    custom_layout = layout_grid_to_dict(custom_layout_grid)\n",
    "    print(custom_layout)\n",
    "    return custom_layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_4\n",
      "2025-03-21_20-18-12\n",
      "LR: 0.05\n",
      "NUM_ENVS: 8\n",
      "NUM_STEPS: 3000\n",
      "TOTAL_TIMESTEPS: 5000000000.0\n",
      "UPDATE_EPOCHS: 1000\n",
      "NUM_MINIBATCHES: 100\n",
      "NUM_ACTORS: 16\n",
      "NUM_UPDATES: 208333\n",
      "MINIBATCH_SIZE: 480\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     44\u001b[39m     save_dir = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minitial_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mSeed_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     45\u001b[39m     train_jit = make_train(config)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     out.append(\u001b[43mtrain_jit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrngs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Extract only the necessary data (returns) and save it\u001b[39;00m\n\u001b[32m     49\u001b[39m results = {\u001b[33m\"\u001b[39m\u001b[33mreturns\u001b[39m\u001b[33m\"\u001b[39m: [out[i][\u001b[33m\"\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mreturned_episode_returns\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[33m\"\u001b[39m\u001b[33mNUM_SEEDS\u001b[39m\u001b[33m\"\u001b[39m])]}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mmake_train.<locals>.train\u001b[39m\u001b[34m(seed, rng, save_dir)\u001b[39m\n\u001b[32m     26\u001b[39m init_x = init_x.flatten()\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Initializes the parameters of the network (network_params) by passing the random key _rng and the input init_x (observation example).\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# This is necessary to set up the weights and biases of the neural network layers.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m network_params = \u001b[43mnetwork\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# This block initializes the optimizer (tx) that will be used to update the network parameters.\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[33m\"\u001b[39m\u001b[33mANNEAL_LR\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/flax/linen/module.py:2452\u001b[39m, in \u001b[36mModule.init\u001b[39m\u001b[34m(self, rngs, method, mutable, capture_intermediates, *args, **kwargs)\u001b[39m\n\u001b[32m   2321\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Initializes a module method with variables and returns modified variables.\u001b[39;00m\n\u001b[32m   2322\u001b[39m \n\u001b[32m   2323\u001b[39m \u001b[33;03m``init`` takes as first argument either a single ``PRNGKey``, or a\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2448\u001b[39m \u001b[33;03m  The initialized variable dict.\u001b[39;00m\n\u001b[32m   2449\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2450\u001b[39m Module._module_checks(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2452\u001b[39m _, v_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_with_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2453\u001b[39m \u001b[43m  \u001b[49m\u001b[43mrngs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2454\u001b[39m \u001b[43m  \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2455\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2456\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmutable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmutable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2457\u001b[39m \u001b[43m  \u001b[49m\u001b[43mcapture_intermediates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapture_intermediates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2458\u001b[39m \u001b[43m  \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2459\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2460\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m v_out\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/flax/linen/module.py:2304\u001b[39m, in \u001b[36mModule.init_with_output\u001b[39m\u001b[34m(self, rngs, method, mutable, capture_intermediates, *args, **kwargs)\u001b[39m\n\u001b[32m   2302\u001b[39m   method = \u001b[38;5;28mself\u001b[39m.\u001b[34m__call__\u001b[39m\n\u001b[32m   2303\u001b[39m method = _get_unbound_fn(method)\n\u001b[32m-> \u001b[39m\u001b[32m2304\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minit_with_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2305\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2306\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2307\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmutable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmutable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2308\u001b[39m \u001b[43m  \u001b[49m\u001b[43mcapture_intermediates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapture_intermediates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2309\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrngs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/flax/core/scope.py:1115\u001b[39m, in \u001b[36minit.<locals>.wrapper\u001b[39m\u001b[34m(rngs, *args, **kwargs)\u001b[39m\n\u001b[32m   1113\u001b[39m   rngs = {\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m: rngs}\n\u001b[32m   1114\u001b[39m init_flags = {**(flags \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}), \u001b[33m'\u001b[39m\u001b[33minitializing\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[32m-> \u001b[39m\u001b[32m1115\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43minit_flags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m  \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrngs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrngs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/flax/core/scope.py:1079\u001b[39m, in \u001b[36mapply.<locals>.wrapper\u001b[39m\u001b[34m(variables, rngs, *args, **kwargs)\u001b[39m\n\u001b[32m   1074\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m errors.ApplyScopeInvalidVariablesStructureError(variables)\n\u001b[32m   1076\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m bind(\n\u001b[32m   1077\u001b[39m   variables, rngs=rngs, mutable=mutable, flags=flags\n\u001b[32m   1078\u001b[39m ).temporary() \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[32m-> \u001b[39m\u001b[32m1079\u001b[39m   y = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mutable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m   1081\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m y, root.mutable_variables()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/flax/linen/module.py:3093\u001b[39m, in \u001b[36minit_with_output.<locals>.scope_fn\u001b[39m\u001b[34m(scope, *args, **kwargs)\u001b[39m\n\u001b[32m   3091\u001b[39m _context.capture_stack.append(capture_intermediates)\n\u001b[32m   3092\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3093\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_deep_clone\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3094\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   3095\u001b[39m   _context.capture_stack.pop()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/flax/linen/module.py:699\u001b[39m, in \u001b[36mwrap_method_once.<locals>.wrapped_module_method\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[32m0\u001b[39m], Module):\n\u001b[32m    698\u001b[39m   \u001b[38;5;28mself\u001b[39m, args = args[\u001b[32m0\u001b[39m], args[\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m699\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_wrapped_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    701\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/flax/linen/module.py:1216\u001b[39m, in \u001b[36mModule._call_wrapped_method\u001b[39m\u001b[34m(self, fun, args, kwargs)\u001b[39m\n\u001b[32m   1214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[32m   1215\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m jax.named_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[32m-> \u001b[39m\u001b[32m1216\u001b[39m     y = \u001b[43mrun_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1218\u001b[39m   y = run_fun(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mActorCritic.__call__\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     15\u001b[39m actor_mean = nn.Dense(\n\u001b[32m     16\u001b[39m     \u001b[32m64\u001b[39m, kernel_init=orthogonal(np.sqrt(\u001b[32m2\u001b[39m)), bias_init=constant(\u001b[32m0.0\u001b[39m)\n\u001b[32m     17\u001b[39m )(actor_mean)\n\u001b[32m     18\u001b[39m actor_mean = activation(actor_mean)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m actor_mean = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_init\u001b[49m\u001b[43m=\u001b[49m\u001b[43morthogonal\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_init\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactor_mean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m pi = distrax.Categorical(logits=actor_mean)\n\u001b[32m     24\u001b[39m critic = nn.Dense(\n\u001b[32m     25\u001b[39m     \u001b[32m64\u001b[39m, kernel_init=orthogonal(np.sqrt(\u001b[32m2\u001b[39m)), bias_init=constant(\u001b[32m0.0\u001b[39m)\n\u001b[32m     26\u001b[39m )(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/flax/linen/module.py:699\u001b[39m, in \u001b[36mwrap_method_once.<locals>.wrapped_module_method\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[32m0\u001b[39m], Module):\n\u001b[32m    698\u001b[39m   \u001b[38;5;28mself\u001b[39m, args = args[\u001b[32m0\u001b[39m], args[\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m699\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_wrapped_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    701\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/flax/linen/module.py:1216\u001b[39m, in \u001b[36mModule._call_wrapped_method\u001b[39m\u001b[34m(self, fun, args, kwargs)\u001b[39m\n\u001b[32m   1214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[32m   1215\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m jax.named_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[32m-> \u001b[39m\u001b[32m1216\u001b[39m     y = \u001b[43mrun_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1218\u001b[39m   y = run_fun(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/flax/linen/linear.py:251\u001b[39m, in \u001b[36mDense.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;129m@compact\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Array) -> Array:\n\u001b[32m    243\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Applies a linear transformation to the inputs along the last dimension.\u001b[39;00m\n\u001b[32m    244\u001b[39m \n\u001b[32m    245\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    249\u001b[39m \u001b[33;03m    The transformed input.\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m   kernel = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mkernel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_bias:\n\u001b[32m    258\u001b[39m     bias = \u001b[38;5;28mself\u001b[39m.param(\n\u001b[32m    259\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mbias\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28mself\u001b[39m.bias_init, (\u001b[38;5;28mself\u001b[39m.features,), \u001b[38;5;28mself\u001b[39m.param_dtype\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/flax/linen/module.py:1877\u001b[39m, in \u001b[36mModule.param\u001b[39m\u001b[34m(self, name, init_fn, unbox, *init_args, **init_kwargs)\u001b[39m\n\u001b[32m   1875\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m errors.NameInUseError(\u001b[33m'\u001b[39m\u001b[33mparam\u001b[39m\u001b[33m'\u001b[39m, name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   1876\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scope \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1877\u001b[39m v = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscope\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munbox\u001b[49m\u001b[43m=\u001b[49m\u001b[43munbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1878\u001b[39m \u001b[38;5;28mself\u001b[39m._state.children[name] = \u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1879\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m v\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/flax/core/scope.py:968\u001b[39m, in \u001b[36mScope.param\u001b[39m\u001b[34m(self, name, init_fn, unbox, *init_args, **init_kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m errors.ScopeCollectionNotFound(\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m, name, \u001b[38;5;28mself\u001b[39m.path_text)\n\u001b[32m    967\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m errors.ScopeParamNotFoundError(name, \u001b[38;5;28mself\u001b[39m.path_text)\n\u001b[32m--> \u001b[39m\u001b[32m968\u001b[39m value = \u001b[43minit_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmake_rng\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mparams\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[38;5;28mself\u001b[39m.put_variable(\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m, name, value)\n\u001b[32m    970\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m unbox:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/jax/_src/nn/initializers.py:613\u001b[39m, in \u001b[36morthogonal.<locals>.init\u001b[39m\u001b[34m(key, shape, dtype)\u001b[39m\n\u001b[32m    611\u001b[39m A = random.normal(key, matrix_shape, dtype)\n\u001b[32m    612\u001b[39m Q, R = jnp.linalg.qr(A)\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m diag_sign = lax.broadcast_to_rank(jnp.sign(\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR\u001b[49m\u001b[43m)\u001b[49m), rank=Q.ndim)\n\u001b[32m    614\u001b[39m Q *= diag_sign \u001b[38;5;66;03m# needed for a uniform distribution\u001b[39;00m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_rows < n_cols: Q = Q.T\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:7976\u001b[39m, in \u001b[36mdiag\u001b[39m\u001b[34m(v, k)\u001b[39m\n\u001b[32m   7928\u001b[39m \u001b[38;5;129m@export\u001b[39m\n\u001b[32m   7929\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdiag\u001b[39m(v: ArrayLike, k: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m) -> Array:\n\u001b[32m   7930\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Returns the specified diagonal or constructs a diagonal array.\u001b[39;00m\n\u001b[32m   7931\u001b[39m \n\u001b[32m   7932\u001b[39m \u001b[33;03m  JAX implementation of :func:`numpy.diag`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   7974\u001b[39m \u001b[33;03m    Array([1, 5, 9], dtype=int32)\u001b[39;00m\n\u001b[32m   7975\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m7976\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_diag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/jax/_src/pjit.py:341\u001b[39m, in \u001b[36m_cpp_pjit.<locals>.cache_miss\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.no_tracing.value:\n\u001b[32m    337\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info.fun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    338\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`jit`, but \u001b[39m\u001b[33m'\u001b[39m\u001b[33mno_tracing\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is set\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    340\u001b[39m (outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked, executable,\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m  pgle_profiler) = \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m maybe_fastpath_data = _get_fastpath_data(\n\u001b[32m    344\u001b[39m     executable, out_tree, args_flat, out_flat, attrs_tracked, jaxpr.effects,\n\u001b[32m    345\u001b[39m     jaxpr.consts, jit_info.abstracted_axes,\n\u001b[32m    346\u001b[39m     pgle_profiler)\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outs, maybe_fastpath_data, _need_to_rebuild_with_fdo(pgle_profiler)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/jax/_src/pjit.py:195\u001b[39m, in \u001b[36m_python_pjit_helper\u001b[39m\u001b[34m(fun, jit_info, *args, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m   args_flat = \u001b[38;5;28mmap\u001b[39m(core.full_lower, args_flat)\n\u001b[32m    194\u001b[39m   core.check_eval_args(args_flat)\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m   out_flat, compiled, profiler = \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    197\u001b[39m   out_flat = pjit_p.bind(*args_flat, **p.params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/jax/_src/pjit.py:1657\u001b[39m, in \u001b[36m_pjit_call_impl_python\u001b[39m\u001b[34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, compiler_options_kvs, *args)\u001b[39m\n\u001b[32m   1645\u001b[39m compiler_options_kvs = compiler_options_kvs + \u001b[38;5;28mtuple\u001b[39m(pgle_compile_options.items())\n\u001b[32m   1646\u001b[39m \u001b[38;5;66;03m# Passing mutable PGLE profile here since it should be extracted by JAXPR to\u001b[39;00m\n\u001b[32m   1647\u001b[39m \u001b[38;5;66;03m# initialize the fdo_profile compile option.\u001b[39;00m\n\u001b[32m   1648\u001b[39m compiled = \u001b[43m_resolve_and_lower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1649\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1652\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[43m    \u001b[49m\u001b[43minline\u001b[49m\u001b[43m=\u001b[49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowering_platforms\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1654\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLoweringParameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1655\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1656\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1659\u001b[39m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n\u001b[32m   1660\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compiled._auto_spmd_lowering \u001b[38;5;129;01mand\u001b[39;00m config.enable_checks.value:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2446\u001b[39m, in \u001b[36mMeshComputation.compile\u001b[39m\u001b[34m(self, compiler_options)\u001b[39m\n\u001b[32m   2444\u001b[39m compiler_options_kvs = \u001b[38;5;28mself\u001b[39m._compiler_options_kvs + t_compiler_options\n\u001b[32m   2445\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options_kvs:\n\u001b[32m-> \u001b[39m\u001b[32m2446\u001b[39m   executable = \u001b[43mUnloadedMeshExecutable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_hlo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2447\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_hlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2448\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2449\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compiler_options_kvs:\n\u001b[32m   2450\u001b[39m     \u001b[38;5;28mself\u001b[39m._executable = executable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2959\u001b[39m, in \u001b[36mUnloadedMeshExecutable.from_hlo\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   2956\u001b[39m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   2958\u001b[39m util.test_event(\u001b[33m\"\u001b[39m\u001b[33mpxla_cached_compilation\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2959\u001b[39m xla_executable = \u001b[43m_cached_compilation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspmd_lowering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtuple_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_spmd_lowering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_prop_to_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_prop_to_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpmap_nreps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2965\u001b[39m orig_out_shardings = out_shardings\n\u001b[32m   2967\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auto_spmd_lowering:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2757\u001b[39m, in \u001b[36m_cached_compilation\u001b[39m\u001b[34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_kvs, pgle_profiler)\u001b[39m\n\u001b[32m   2749\u001b[39m compile_options = create_compile_options(\n\u001b[32m   2750\u001b[39m     computation, mesh, spmd_lowering, tuple_args, auto_spmd_lowering,\n\u001b[32m   2751\u001b[39m     allow_prop_to_inputs, allow_prop_to_outputs, backend,\n\u001b[32m   2752\u001b[39m     dev, pmap_nreps, compiler_options)\n\u001b[32m   2754\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dispatch.log_elapsed_time(\n\u001b[32m   2755\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{elapsed_time:.9f}\u001b[39;00m\u001b[33m sec\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2756\u001b[39m     fun_name=name, event=dispatch.BACKEND_COMPILE_EVENT):\n\u001b[32m-> \u001b[39m\u001b[32m2757\u001b[39m   xla_executable = \u001b[43mcompiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_or_get_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2758\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2759\u001b[39m \u001b[43m      \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2760\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/jax/_src/compiler.py:473\u001b[39m, in \u001b[36mcompile_or_get_cached\u001b[39m\u001b[34m(backend, computation, devices, compile_options, host_callbacks, pgle_profiler)\u001b[39m\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    472\u001b[39m   log_persistent_cache_miss(module_name, cache_key)\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_and_write_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m      \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/jax/_src/compiler.py:690\u001b[39m, in \u001b[36m_compile_and_write_cache\u001b[39m\u001b[34m(backend, computation, compile_options, host_callbacks, module_name, cache_key)\u001b[39m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_compile_and_write_cache\u001b[39m(\n\u001b[32m    682\u001b[39m     backend: xc.Client,\n\u001b[32m    683\u001b[39m     computation: ir.Module,\n\u001b[32m   (...)\u001b[39m\u001b[32m    687\u001b[39m     cache_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    688\u001b[39m ) -> xc.LoadedExecutable:\n\u001b[32m    689\u001b[39m   start_time = time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m690\u001b[39m   executable = \u001b[43mbackend_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    693\u001b[39m   compile_time = time.monotonic() - start_time\n\u001b[32m    694\u001b[39m   _cache_write(\n\u001b[32m    695\u001b[39m       cache_key, compile_time, module_name, backend, executable, host_callbacks\n\u001b[32m    696\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/jax/_src/profiler.py:334\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    333\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    335\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/JaxMARL/lib/python3.12/site-packages/jax/_src/compiler.py:324\u001b[39m, in \u001b[36mbackend_compile\u001b[39m\u001b[34m(backend, module, options, host_callbacks)\u001b[39m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m backend.compile(\n\u001b[32m    319\u001b[39m         built_c, compile_options=options, host_callbacks=host_callbacks\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m   \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[32m    322\u001b[39m   \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[32m    323\u001b[39m   \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m xc.XlaRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    326\u001b[39m   \u001b[38;5;28;01mfor\u001b[39;00m error_handler \u001b[38;5;129;01min\u001b[39;00m _XLA_RUNTIME_ERROR_HANDLERS:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "layout_name = \"custom_1\"\n",
    "\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "initial_dir = f'/data/samuel_lozano/hfsp_collective_learning/data_JaxMARL/Symmetric_Agents/{layout_name}/Checkpoints_{current_datetime}/'\n",
    "\n",
    "print(layout_name)\n",
    "print(current_datetime)\n",
    "custom_layout = load_custom_layout(layout_name)\n",
    "\n",
    "# set hyperparameters:\n",
    "config = {\n",
    "    # Number of possible actions in the environment. \n",
    "    \"NUM_ACTIONS\": 6, \n",
    "    # Controls how much the model updates its weights during optimization.\n",
    "    \"LR\": 1e-4, \n",
    "    # Number of parallel environments running simultaneously.\n",
    "    \"NUM_ENVS\": 8, \n",
    "    #Number of steps collected before running an update.\n",
    "    \"NUM_STEPS\": 1500, \n",
    "    # Total number of timesteps for training.\n",
    "    \"TOTAL_TIMESTEPS\": 5e8, \n",
    "    # Number of times each collected batch of experiences is used for gradient updates. More epochs allow for better learning from collected data but can lead to overfitting.\n",
    "    \"UPDATE_EPOCHS\": 4, \n",
    "    # Number of minibatches used during training updates. More minibatches reduce variance but increase computational cost.\n",
    "    \"NUM_MINIBATCHES\": 4, \n",
    "    # Discount factor for future rewards. A value close to 1 favors long-term rewards, while lower values prioritize immediate rewards.\n",
    "    \"GAMMA\": 0.99, \n",
    "    # Controls Generalized Advantage Estimation (GAE). Higher values lead to smoother, less biased advantage estimates.\n",
    "    \"GAE_LAMBDA\": 0.99, \n",
    "    # PPO-specific parameter that limits policy updates to prevent excessive changes. Lower values ensure stability but may slow training.\n",
    "    \"CLIP_EPS\": 0.2, \n",
    "    # Coefficient for entropy regularization, encouraging exploration. Higher values lead to more randomness in actions.\n",
    "    \"ENT_COEF\": 0.1, \n",
    "    # Coefficient for the value function loss. Higher values prioritize accurate value function learning.\n",
    "    \"VF_COEF\": 0.5, \n",
    "    # Limits the magnitude of gradients, preventing instability.\n",
    "    \"MAX_GRAD_NORM\": 0.5, \n",
    "    # Specifies the activation function for the network. Tanh helps with stable gradient flow but might limit expressiveness compared to ReLU.\n",
    "    \"ACTIVATION\": \"tanh\",\n",
    "    # The RL environment being used.\n",
    "    \"ENV_NAME\": \"overcooked\",\n",
    "    # Allows customization of environment settings, such as custom layouts.\n",
    "    \"ENV_KWARGS\": {\n",
    "      \"layout\" : custom_layout #write custom_layout without quotation marks if you want to use the custom layout\n",
    "    },\n",
    "    # If enabled, the learning rate decreases over time, improving stability in later training stages.\n",
    "    \"ANNEAL_LR\": True, \n",
    "    # Ensures reproducibility by fixing the random seed.\n",
    "    \"SEED\": 0,\n",
    "    # Runs multiple training instances with different seeds for robustness.\n",
    "    \"NUM_SEEDS\": 3,\n",
    "    # Saves model checkpoints every N epochs.\n",
    "    \"SAVE_EVERY_N_EPOCHS\": 50,\n",
    "}\n",
    "\n",
    "#Comment the following line if you want to use the custom_layout\n",
    "#config[\"ENV_KWARGS\"][\"layout\"] = overcooked_layouts[config[\"ENV_KWARGS\"][\"layout\"]]\n",
    "\n",
    "config = make_config(config)\n",
    "\n",
    "rng = jax.random.PRNGKey(config[\"SEED\"])\n",
    "rngs = jax.random.split(rng, config[\"NUM_SEEDS\"])\n",
    "\n",
    "out=[]\n",
    "# Training loop\n",
    "for seed in range(config[\"NUM_SEEDS\"]):\n",
    "    save_dir = f'{initial_dir}Seed_{seed}/'\n",
    "    train_jit = make_train(config)\n",
    "    out.append(train_jit(seed, rngs[seed], save_dir))\n",
    "\n",
    "# Extract only the necessary data (returns) and save it\n",
    "results = {\"returns\": [out[i][\"metrics\"][\"returned_episode_returns\"] for i in range(config[\"NUM_SEEDS\"])]}\n",
    "np.savez(os.path.join(initial_dir, \"plot_data.npz\"), **results)\n",
    "print(f\"Data saved in {initial_dir}/plot_data.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "for i in range(config[\"NUM_SEEDS\"]):\n",
    "    returns = jnp.stack(out[i][\"metrics\"][\"returned_episode_returns\"])  # Stack list into a single array\n",
    "    mean_returns = returns.mean(axis=-1).reshape(-1)  # Compute mean across the stacked dimension\n",
    "    plt.plot(mean_returns, label=f\"Seed {i}\")\n",
    "plt.xlabel(\"Update Step\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "\n",
    "# Guardar la imagen en el directorio especificado\n",
    "save_path = os.path.join(initial_dir, \"plot.png\")\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# Opcional: mostrar la imagen\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = int(0.17e6)  # Convertir timesteps a update steps\n",
    "\n",
    "plt.figure(figsize=(10, 5))  # Opcional: Ajustar tamaño del gráfico\n",
    "\n",
    "for i in range(config[\"NUM_SEEDS\"]):\n",
    "    returns = jnp.stack(out[i][\"metrics\"][\"returned_episode_returns\"])  \n",
    "    mean_returns = returns.mean(axis=-1).reshape(-1)\n",
    "    \n",
    "    # Graficar solo hasta max_steps\n",
    "    plt.plot(mean_returns[:max_steps], label=f\"Seed {i}\")\n",
    "\n",
    "plt.xlabel(\"Update Step\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "\n",
    "# Guardar la imagen\n",
    "save_path = os.path.join(initial_dir, \"plot_zoom.png\")\n",
    "plt.savefig(save_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_return_per_seed = {}\n",
    "\n",
    "for i in range(config[\"NUM_SEEDS\"]):\n",
    "    returns = jnp.stack(out[i][\"metrics\"][\"returned_episode_returns\"])\n",
    "    mean_returns = returns.mean(axis=-1).reshape(-1)\n",
    "    \n",
    "    max_index = jnp.argmax(mean_returns)  # Índice del máximo retorno\n",
    "    max_step = max_index / config[\"NUM_STEPS\"]  # Convertir a update steps\n",
    "    max_value = mean_returns[max_index]  # Obtener el valor máximo\n",
    "    \n",
    "    max_return_per_seed[f\"Seed {i}\"] = (int(max_step), float(max_value))  # Guardar en diccionario\n",
    "\n",
    "print(max_return_per_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RECREATING RESULTS OF TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"NUM_ACTIONS\": 6,\n",
    "    \"LR\": 1e-2,\n",
    "    \"NUM_ENVS\": 8,\n",
    "    \"NUM_STEPS\": 1000,\n",
    "    \"TOTAL_TIMESTEPS\": 6e7,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": 4,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.99,\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"ENT_COEF\": 0.01,\n",
    "    \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"tanh\",\n",
    "    \"ENV_NAME\": \"overcooked\",\n",
    "    \"ENV_KWARGS\": {\n",
    "      \"layout\" : \"custom_layout\" #write custom_layout without quotation marks if you want to use the custom layout\n",
    "    },\n",
    "    \"ANNEAL_LR\": True,\n",
    "    \"SEED\": 0,\n",
    "    \"NUM_SAVES\": 1000,\n",
    "    \"NUM_SEEDS\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max steps per seed (reconstructed):\n",
      "{'Seed 0': (1804.399, 200.0), 'Seed 1': (1670.799, 220.0), 'Seed 2': (17607.999, 240.0)}\n"
     ]
    }
   ],
   "source": [
    "#OLD PLOT_DATA\n",
    "\n",
    "#REVISAR\n",
    "\n",
    "# Recreate values and figure of evolution\n",
    "def OLD_recreate_results(data_path, figure = 1):\n",
    "    data = np.load(data_path, allow_pickle=True)\n",
    "    returns_list = data[\"returns\"]\n",
    "\n",
    "    # 🔹 Reconstruct `out`\n",
    "    out_reconstructed = [{\"metrics\": {\"returned_episode_returns\": returns}} for returns in returns_list]\n",
    "\n",
    "    # 🔹 Compute `max_steps_per_seed`\n",
    "    max_steps_per_seed = {}\n",
    "    mean_returns_per_seed = [] \n",
    "\n",
    "    for i in range(len(out_reconstructed)):\n",
    "        returns = jnp.stack(out_reconstructed[i][\"metrics\"][\"returned_episode_returns\"])\n",
    "        mean_returns = returns.mean(axis=-1).reshape(-1)\n",
    "\n",
    "        mean_returns_per_seed.append(mean_returns)\n",
    "        \n",
    "        max_index = int(jnp.argmax(mean_returns))  # Index of max return\n",
    "        max_step = max_index / config[\"NUM_STEPS\"]  # Convert to update steps\n",
    "        max_value = float(mean_returns[max_index])  # Get max value\n",
    "        \n",
    "        max_steps_per_seed[f\"Seed {i}\"] = (max_step, max_value)\n",
    "\n",
    "    # 🔹 Recreate the plot\n",
    "    if figure == 1:\n",
    "        plt.figure()\n",
    "        for i in range(len(out_reconstructed)):\n",
    "            plt.plot(mean_returns_per_seed[i], label=f\"Seed {i}\")\n",
    "    \n",
    "        plt.xlabel(\"Update Step\")\n",
    "        plt.ylabel(\"Return\")\n",
    "        plt.legend()\n",
    "    \n",
    "        save_path = os.path.join(os.path.dirname(data_path), \"plot.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.show()\n",
    "\n",
    "    print(\"Max steps per seed (reconstructed):\")\n",
    "    print(max_steps_per_seed)\n",
    "    return mean_returns_per_seed, max_steps_per_seed\n",
    "\n",
    "load_datetime = '2025-03-17_10-32-29'\n",
    "load_layout_name = 'custom_2'\n",
    "\n",
    "original_dir = f'/data/samuel_lozano/hfsp_collective_learning/data_JaxMARL/Symmetric_Agents/{load_layout_name}/Checkpoints_{load_datetime}/'\n",
    "\n",
    "print_figure = 0\n",
    "\n",
    "mean_returns_per_seed, max_steps_per_seed = OLD_recreate_results(os.path.join(original_dir, \"plot_data.npz\"), figure = print_figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW REVISAR\n",
    "\n",
    "def recreate_results(directory, figure=1):\n",
    "    # Find all files matching the pattern \"plot_data_seed_{seed}.npz\"\n",
    "    seed_files = [f for f in os.listdir(directory) if re.match(r\"plot_data_seed_\\d+\\.npz\", f)]\n",
    "    seed_files.sort(key=lambda x: int(re.findall(r\"\\d+\", x)[0]))  # Sort by seed number\n",
    "\n",
    "    if not seed_files:\n",
    "        print(\"No seed files found!\")\n",
    "        return None, None, None\n",
    "    \n",
    "    max_steps_per_seed = {}\n",
    "    mean_returns_per_seed = [] \n",
    "\n",
    "    i = 0\n",
    "    for file in seed_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        returns_list = jnp.expand_dims(data[\"returns\"], axis=0)\n",
    "        out_reconstructed = [{\"metrics\": {\"returned_episode_returns\": returns}} for returns in returns_list]\n",
    "    \n",
    "        returns = jnp.stack(out_reconstructed[0][\"metrics\"][\"returned_episode_returns\"])\n",
    "        mean_returns = returns.mean(axis=-1).reshape(-1)\n",
    "\n",
    "        mean_returns_per_seed.append(mean_returns)\n",
    "        \n",
    "        max_index = int(jnp.argmax(mean_returns))  # Index of max return\n",
    "        max_step = max_index / config[\"NUM_STEPS\"]  # Convert to update steps\n",
    "        max_value = float(mean_returns[max_index])  # Get max value\n",
    "        \n",
    "        max_steps_per_seed[f\"Seed {i}\"] = (max_step, max_value)\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    # Recreate the plot\n",
    "    if figure == 1:\n",
    "        plt.figure()\n",
    "        for i in range(len(seed_files)):\n",
    "            plt.plot(mean_returns_per_seed[i], label=f\"Seed {i}\")\n",
    "    \n",
    "        plt.xlabel(\"Update Step\")\n",
    "        plt.ylabel(\"Return\")\n",
    "        plt.legend()\n",
    "    \n",
    "        save_path = os.path.join(directory, \"plot.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.show()\n",
    "\n",
    "    print(\"Max steps per seed (reconstructed):\")\n",
    "    print(max_steps_per_seed)\n",
    "    return mean_returns_per_seed, max_steps_per_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max steps per seed (reconstructed):\n",
      "{'Seed 0': (8874.799, 180.0), 'Seed 1': (12084.799, 180.0), 'Seed 2': (8452.799, 180.0)}\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the seed files\n",
    "load_datetime = '2025-03-18_15-22-00'\n",
    "load_layout_name = 'custom_5'\n",
    "original_dir = f'/data/samuel_lozano/hfsp_collective_learning/data_JaxMARL/Symmetric_Agents/{load_layout_name}/Checkpoints_{load_datetime}/'\n",
    "\n",
    "# Run the function\n",
    "print_figure = 0\n",
    "mean_returns_per_seed, max_steps_per_seed = recreate_results(original_dir, figure=print_figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = int(0.08e8)  # Convertir timesteps a update steps\n",
    "\n",
    "plt.figure(figsize=(10, 5))  # Opcional: Ajustar tamaño del gráfico\n",
    "\n",
    "for i in range(config[\"NUM_SEEDS\"]):    \n",
    "    # Graficar solo hasta max_steps\n",
    "    plt.plot(mean_returns[:max_steps], label=f\"Seed {i}\")\n",
    "\n",
    "plt.xlabel(\"Update Step\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "\n",
    "# Guardar la imagen\n",
    "save_path = os.path.join(original_dir, \"plot_zoom.png\")\n",
    "plt.savefig(save_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_value_return_interval(seed, mean_returns_per_seed, config, searched_value, min_step, max_step):\n",
    "    for step in range(int(min_step), int(max_step + 1)):\n",
    "        scaled_step = step // config[\"NUM_STEPS\"]  # Adjust step according to NUM_STEPS\n",
    "        if mean_returns_per_seed[seed][step] == searched_value:\n",
    "            print(f\"First return value {searched_value} in seed {seed} at update_step: {step}. In update step: {scaled_step}\")\n",
    "            return scaled_step\n",
    "\n",
    "    print(f\"No return value {searched_value} found in seed {seed} within the specified range.\")\n",
    "    return None\n",
    "\n",
    "def find_different_value_return_interval(seed, mean_returns_per_seed, config, searched_value, min_step, max_step):\n",
    "    for step in range(int(min_step), int(max_step + 1)):\n",
    "        scaled_step = step // config[\"NUM_STEPS\"]  # Adjust step according to NUM_STEPS\n",
    "        if mean_returns_per_seed[seed][step] != searched_value:\n",
    "            print(f\"First return value {searched_value} in seed {seed} at update_step: {step}. In update step: {scaled_step}\")\n",
    "            return scaled_step\n",
    "\n",
    "    print(f\"No return value {searched_value} found in seed {seed} within the specified range.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First return value 0 in seed 2 at update_step: 4063599. In update step: 4063\n"
     ]
    }
   ],
   "source": [
    "seed = 2\n",
    "searched_value = 0\n",
    "min_step = 4e6\n",
    "max_step = 4.3e6\n",
    "\n",
    "found_step = find_value_return_interval(seed, mean_returns_per_seed, config, searched_value, min_step, max_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_steps(original_dir, config, mean_returns):\n",
    "    seed_pattern = re.compile(r\"Seed_(\\d+)\")\n",
    "    step_pattern = re.compile(r\"trained_model_(\\d+)\\.pkl\")\n",
    "\n",
    "    results = {}  # Final dictionary with the reconstructed values\n",
    "\n",
    "    for seed_folder in os.listdir(original_dir):\n",
    "        seed_match = seed_pattern.match(seed_folder)\n",
    "        if seed_match:\n",
    "            seed = int(seed_match.group(1))\n",
    "            seed_path = os.path.join(original_dir, seed_folder)\n",
    "            \n",
    "            if os.path.isdir(seed_path):\n",
    "                steps = []\n",
    "                for file in os.listdir(seed_path):\n",
    "                    step_match = step_pattern.match(file)\n",
    "                    if step_match:\n",
    "                        steps.append(int(step_match.group(1)))\n",
    "\n",
    "                steps.sort() \n",
    "                results[seed] = {}\n",
    "\n",
    "                for step in steps:\n",
    "                    scaled_step = step * config[\"NUM_STEPS\"]                   \n",
    "                    results[seed][step] = mean_returns[scaled_step]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_seed = get_trained_steps(original_dir, config, mean_returns)\n",
    "print(steps_per_seed[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING AND EVALUATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict({\n",
      "    wall_idx: Array([ 4,  5,  6,  7,  8, 13, 15, 17, 22, 24, 26, 31, 33, 35, 40, 41, 42,\n",
      "           43, 44], dtype=int32),\n",
      "    agent_idx: Array([23, 25], dtype=int32),\n",
      "    goal_idx: Array([41, 43], dtype=int32),\n",
      "    plate_pile_idx: Array([15], dtype=int32),\n",
      "    onion_pile_idx: Array([33], dtype=int32),\n",
      "    pot_idx: Array([5, 7], dtype=int32),\n",
      "    height: 6,\n",
      "    width: 9,\n",
      "})\n",
      "LR: 0.01\n",
      "NUM_ENVS: 8\n",
      "NUM_STEPS: 1000\n",
      "TOTAL_TIMESTEPS: 200000000.0\n",
      "UPDATE_EPOCHS: 4\n",
      "NUM_MINIBATCHES: 4\n",
      "NUM_ACTORS: 16\n",
      "NUM_UPDATES: 25000\n",
      "MINIBATCH_SIZE: 4000\n",
      "Found checkpoint: trained_model_24999.pkl for step 24999\n",
      "Model parameters loaded from /data/samuel_lozano/hfsp_collective_learning/data_JaxMARL/custom_5/Checkpoints_2025-03-18_15-22-00/Seed_0/trained_model_24999.pkl\n",
      "Model restored successfully from step 24999\n",
      "with mean return 180.0!\n"
     ]
    }
   ],
   "source": [
    "fixed_step = 25000  # Change this to the desired step\n",
    "\n",
    "seed_idx = 0  # Change if needed\n",
    "load_datetime = '2025-03-18_15-22-00'\n",
    "load_layout_name = 'custom_5'\n",
    "custom_layout = load_custom_layout(load_layout_name)\n",
    "\n",
    "original_dir = f'/data/samuel_lozano/hfsp_collective_learning/data_JaxMARL/Symmetric_Agents/{load_layout_name}/Checkpoints_{load_datetime}/'\n",
    "load_dir = f'{original_dir}Seed_{seed_idx}/'\n",
    "load_filename = f\"trained_model_{fixed_step}.pkl\"\n",
    "\n",
    "# set hyperparameters:\n",
    "config = {\n",
    "    \"NUM_ACTIONS\": 6,\n",
    "    \"LR\": 1e-2,\n",
    "    \"NUM_ENVS\": 8,\n",
    "    \"NUM_STEPS\": 1000,\n",
    "    \"TOTAL_TIMESTEPS\": 2e8,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": 4,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.99,\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"ENT_COEF\": 0.01,\n",
    "    \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"tanh\",\n",
    "    \"ENV_NAME\": \"overcooked\",\n",
    "    \"ENV_KWARGS\": {\n",
    "      \"layout\" : custom_layout #write custom_layout without quotation marks if you want to use the custom layout\n",
    "    },\n",
    "    \"ANNEAL_LR\": True,\n",
    "    \"SEED\": 0,\n",
    "    \"NUM_SAVES\": 500,\n",
    "    \"NUM_SEEDS\": 3\n",
    "}\n",
    "#Comment the following line if you want to use the custom_layout\n",
    "#config[\"ENV_KWARGS\"][\"layout\"] = overcooked_layouts[config[\"ENV_KWARGS\"][\"layout\"]]\n",
    "\n",
    "config = make_config(config)\n",
    "\n",
    "# Initialize network\n",
    "network = ActorCritic(config[\"NUM_ACTIONS\"], activation=config[\"ACTIVATION\"])\n",
    "\n",
    "# Load parameters into the network\n",
    "loaded_params, closest_step = find_closest_checkpoint(fixed_step, load_dir)\n",
    "    \n",
    "# Create the train state with loaded parameters\n",
    "train_state = TrainState.create(\n",
    "    apply_fn=network.apply,\n",
    "    params=loaded_params,  # Restored parameters\n",
    "    tx=choose_tx(config),\n",
    ")\n",
    "\n",
    "print(f\"Model restored successfully from step {closest_step}\")\n",
    "print(f\"with mean return {float(mean_returns_per_seed[seed_idx][closest_step * config['NUM_STEPS']])}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of agents in environment: ['agent_0', 'agent_1']\n"
     ]
    }
   ],
   "source": [
    "# Set environment parameters\n",
    "max_steps = 1000\n",
    "key = jax.random.PRNGKey(0)\n",
    "key, key_r, key_a = jax.random.split(key, 3)\n",
    "\n",
    "# Choose layout\n",
    "#layout = overcooked_layouts[\"cramped_room\"]\n",
    "layout = custom_layout\n",
    "\n",
    "# Instantiate environment\n",
    "env = make('overcooked', layout=layout, max_steps=max_steps)\n",
    "\n",
    "# Reset environment\n",
    "obs, state = env.reset(key_r)\n",
    "print('List of agents in environment:', env.agents)\n",
    "\n",
    "# Visualization setup\n",
    "viz = OvercookedVisualizer()\n",
    "state_seq = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run environment loop using the trained model\n",
    "for _ in range(max_steps):\n",
    "    state_seq.append(state)\n",
    "    \n",
    "    # Get model-based actions\n",
    "    key, key_s = jax.random.split(key, 2)\n",
    "    \n",
    "    actions = {}\n",
    "    for i, agent in enumerate(env.agents):\n",
    "        agent_obs = obs[agent]  # Extract observation for each agent\n",
    "        action_logits, value = network.apply(train_state.params, agent_obs.flatten())  # Get model's action distribution\n",
    "        action = action_logits.sample(seed=key_s)\n",
    "        key, key_s = jax.random.split(key)\n",
    "        actions[agent] = action\n",
    "    \n",
    "    # Step environment\n",
    "    obs, state, rewards, dones, infos = env.step(key_s, state, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animation saved to /data/samuel_lozano/hfsp_collective_learning/data_JaxMARL/custom_5/Checkpoints_2025-03-18_15-22-00/Seed_0//animation_trained_model_24999_max_steps_1000.gif with adjusted speed\n",
      "MP4 saved to /data/samuel_lozano/hfsp_collective_learning/data_JaxMARL/custom_5/Checkpoints_2025-03-18_15-22-00/Seed_0//animation_trained_model_24999_max_steps_1000.mp4\n"
     ]
    }
   ],
   "source": [
    "# FIX VISUALIZATION (SAVING IT WORKS)\n",
    "\n",
    "# Render to screen\n",
    "#for s in state_seq:\n",
    "#    viz.render(env.agent_view_size, s, highlight=False)\n",
    "#    time.sleep(0.1)\n",
    "\n",
    "# Save animation\n",
    "agent_view_size = 5\n",
    "\n",
    "output_filename = f\"{load_dir}/animation_trained_model_{closest_step}_max_steps_{max_steps}.gif\"\n",
    "\n",
    "custom_animate(state_seq, agent_view_size=agent_view_size, filename=output_filename)\n",
    "\n",
    "print(f\"Animation saved to {output_filename} with adjusted speed\")\n",
    "\n",
    "gif_to_mp4(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JaxMARL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
