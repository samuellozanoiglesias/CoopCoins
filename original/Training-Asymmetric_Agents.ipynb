{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX is using: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"JAX_PLATFORMS\"] = \"cpu\"  # Forces JAX to run on CPU\n",
    "\n",
    "import jax #pip install jax\n",
    "print(f\"JAX is using: {jax.devices()[0]}\")\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax #pip install distrax\n",
    "from gymnax.wrappers.purerl import LogWrapper, FlattenObservationWrapper\n",
    "import jaxmarl\n",
    "from jaxmarl.wrappers.baselines import LogWrapper\n",
    "from jaxmarl.environments.overcooked import Overcooked, overcooked_layouts, layout_grid_to_dict\n",
    "from jaxmarl.viz.overcooked_visualizer import OvercookedVisualizer\n",
    "from jaxmarl import make\n",
    "import hydra #pip install hydra-core\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Added to modify the structure of the definitions\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.lax as lax\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import time\n",
    "import re\n",
    "from textwrap import dedent\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Save the model parameters only\n",
    "def save_model(runner_state, save_dir, file):\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Construct the full file path\n",
    "    filename = os.path.join(save_dir, file)\n",
    "\n",
    "    # Extract train_state (first element of the tuple)\n",
    "    train_state_1 = runner_state[0]\n",
    "    train_state_2 = runner_state[1]\n",
    "\n",
    "    # Save only the model parameters\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump({'params_1': train_state_1.params, \n",
    "                     'params_2': train_state_2.params}, f)\n",
    "\n",
    "\n",
    "    print(f\"Model parameters saved to {filename}\")\n",
    "\n",
    "# Load the model parameters\n",
    "def load_model(load_dir, file):\n",
    "    filename = os.path.join(load_dir, file)\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(f\"Model file {filename} not found.\")\n",
    "\n",
    "    with open(filename, \"rb\") as f:\n",
    "        saved_data = pickle.load(f)\n",
    "\n",
    "    # Ensure the loaded model has the correct parameter structure\n",
    "    if \"params_1\" not in saved_data or \"params_2\" not in saved_data:\n",
    "        raise ValueError(\"Invalid saved model file.\")\n",
    "\n",
    "    model_params_1 = saved_data[\"params_1\"]\n",
    "    model_params_2 = saved_data[\"params_2\"]\n",
    "    print(f\"Model parameters loaded from {filename}\")\n",
    "\n",
    "    return model_params_1, model_params_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_tx(config):\n",
    "    def linear_schedule(count):\n",
    "        frac = 1.0 - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"])) / config[\"NUM_UPDATES\"]\n",
    "        return config[\"LR\"] * frac\n",
    "    \n",
    "    if config[\"ANNEAL_LR\"]:\n",
    "        tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "        )\n",
    "    else:\n",
    "        tx = optax.chain(optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]), optax.adam(config[\"LR\"], eps=1e-5))\n",
    "    \n",
    "    return tx\n",
    "\n",
    "def find_closest_checkpoint(fixed_step, load_dir):\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(load_dir)\n",
    "    \n",
    "    # Filter files that match the pattern 'trained_model_{step}.pkl'\n",
    "    checkpoint_files = [f for f in files if f.startswith(\"trained_model_\") and f.endswith(\".pkl\")]\n",
    "    \n",
    "    # Extract the step numbers from the filenames\n",
    "    steps = []\n",
    "    for file in checkpoint_files:\n",
    "        step_str = file.split('_')[-1].split('.')[0]  # Extract the step number\n",
    "        try:\n",
    "            step = int(step_str)\n",
    "            steps.append(step)\n",
    "        except ValueError:\n",
    "            continue  # Skip files that don't have a valid step number\n",
    "    \n",
    "    if len(steps) == 0:\n",
    "        raise FileNotFoundError(f\"No checkpoint files found in {load_dir}\")\n",
    "    \n",
    "    # Find the closest step to the fixed step\n",
    "    closest_step = min(steps, key=lambda x: abs(x - fixed_step))\n",
    "    closest_filename = f\"trained_model_{closest_step}.pkl\"\n",
    "    \n",
    "    print(f\"Found checkpoint: {closest_filename} for step {closest_step}\")\n",
    "    \n",
    "    # Load the model (assuming load_model function is available)\n",
    "    model_params_1, model_params_2 = load_model(load_dir, closest_filename)\n",
    "    \n",
    "    return model_params_1, model_params_2, closest_step\n",
    "\n",
    "import imageio.v3 as iio\n",
    "\n",
    "def custom_animate(state_seq, agent_view_size, filename=\"animation.gif\"):\n",
    "    \"\"\"Animate a GIF give a state sequence and save if to file.\"\"\"\n",
    "    import imageio\n",
    "    TILE_PIXELS = 32\n",
    "\n",
    "    padding = agent_view_size - 2  # show\n",
    "\n",
    "    def get_frame(state):\n",
    "            grid = np.asarray(state.maze_map[padding:-padding, padding:-padding, :])\n",
    "            # Render the state\n",
    "            frame = OvercookedVisualizer._render_grid(\n",
    "                    grid,\n",
    "                    tile_size=TILE_PIXELS,\n",
    "                    highlight_mask=None,\n",
    "                    agent_dir_idx=state.agent_dir_idx,\n",
    "                    agent_inv=state.agent_inv\n",
    "            )\n",
    "            return frame\n",
    "\n",
    "    frame_seq =[get_frame(state) for state in state_seq]\n",
    "    \n",
    "    imageio.mimsave(filename, frame_seq, 'GIF', duration=2)\n",
    "    \n",
    "def gif_to_mp4(gif_filename):\n",
    "    \"\"\"Convert a GIF to MP4 using imageio and ffmpeg.\"\"\"\n",
    "    mp4_filename = gif_filename.replace(\".gif\", \".mp4\")\n",
    "    gif = iio.imread(gif_filename)\n",
    "    fps = 2  # Set desired FPS\n",
    "\n",
    "    iio.imwrite(mp4_filename, gif, extension=\".mp4\", fps=fps)\n",
    "    print(f\"MP4 saved to {mp4_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        pi = distrax.Categorical(logits=actor_mean)\n",
    "\n",
    "        critic = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(\n",
    "            64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)\n",
    "        )(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
    "            critic\n",
    "        )\n",
    "\n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    info: jnp.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_info(info):\n",
    "    def reshape_fn(x):\n",
    "        if isinstance(x, dict):\n",
    "            return jax.tree.map(lambda y: y.reshape(-1), x)\n",
    "        # For (8, 2) shaped arrays, reshape to (16,)\n",
    "        elif len(x.shape) == 2 and x.shape[1] == 2:\n",
    "            return x.reshape(-1)\n",
    "        # For (8,) shaped arrays\n",
    "        else:\n",
    "            # Repeat each element twice since we have 2 agents\n",
    "            return jnp.repeat(x, 2)\n",
    "    return jax.tree.map(reshape_fn, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_info(info):\n",
    "    def split_fn_agent_0(x):\n",
    "        if isinstance(x, dict):\n",
    "            return jax.tree.map(lambda y: y[:, 0], x)\n",
    "        # For (8, 2) shaped arrays, reshape to (16,)\n",
    "        elif len(x.shape) == 2 and x.shape[1] == 2:\n",
    "            return x[:, 0]\n",
    "        # For (8,) shaped arrays\n",
    "        else:\n",
    "            # Repeat each element twice since we have 2 agents\n",
    "            return x\n",
    "\n",
    "    def split_fn_agent_1(x):\n",
    "        if isinstance(x, dict):\n",
    "            return jax.tree.map(lambda y: y[:, 1], x)\n",
    "        # For (8, 2) shaped arrays, reshape to (16,)\n",
    "        elif len(x.shape) == 2 and x.shape[1] == 2:\n",
    "            return x[:, 1]\n",
    "        # For (8,) shaped arrays\n",
    "        else:\n",
    "            # Repeat each element twice since we have 2 agents\n",
    "            return x\n",
    "\n",
    "    return jax.tree.map(split_fn_agent_0, info), jax.tree.map(split_fn_agent_1, info)\n",
    "\n",
    "\n",
    "def merge_info(info_1, info_2):\n",
    "    def merge_fn(x_1, x_2):\n",
    "        if isinstance(x_1, dict):\n",
    "            # If the values are dictionaries, recursively merge them\n",
    "            return jax.tree.map(merge_fn, x_1, x_2)\n",
    "        else:\n",
    "            # For array values, stack them along a new axis (axis=1)\n",
    "            return jnp.stack([x_1, x_2], axis=1)\n",
    "\n",
    "    # Use tree map to merge corresponding elements from both dictionaries\n",
    "    return jax.tree.map(merge_fn, info_1, info_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(x: dict, agent_list, num_actors):\n",
    "    x = jnp.stack([x[a] for a in agent_list])\n",
    "    return x.reshape((num_actors, -1))\n",
    "\n",
    "def unbatchify(x: jnp.ndarray, agent_list, num_envs, num_actors):\n",
    "    x = x.reshape((num_actors, num_envs, -1))\n",
    "    return {a: x[i] for i, a in enumerate(agent_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_config(config):\n",
    "    \"\"\"Enhances config with additional computed parameters and prints it in a readable format.\"\"\"\n",
    "    env = jaxmarl.make(config[\"ENV_NAME\"], **config[\"ENV_KWARGS\"])\n",
    "\n",
    "    # Compute additional parameters\n",
    "    config[\"NUM_ACTORS\"] = env.num_agents * config[\"NUM_ENVS\"]\n",
    "    config[\"NUM_UPDATES\"] = int(config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"])\n",
    "    config[\"NUM_SAVES\"] = int(config[\"NUM_UPDATES\"] // config[\"SAVE_EVERY_N_EPOCHS\"])\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        int(config[\"NUM_ACTORS\"] / 2) * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "\n",
    "    # Print all configuration settings in a structured way\n",
    "    print(\"\\n===== CONFIGURATION SETTINGS =====\")\n",
    "    for key in sorted(config.keys()):  # Sort keys alphabetically for readability\n",
    "        print(f\"{key}: {config[key]}\")\n",
    "    print(\"==================================\\n\")\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train(config):\n",
    "    env = jaxmarl.make(config[\"ENV_NAME\"], **config[\"ENV_KWARGS\"])\n",
    "\n",
    "    env = LogWrapper(env)\n",
    "\n",
    "    def linear_schedule(count):\n",
    "        frac = 1.0 - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"])) / config[\"NUM_UPDATES\"]\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def train(seed, rng, initial_dir, csv_file_path):\n",
    "\n",
    "        # INIT NETWORK\n",
    "\n",
    "        # Creates an instance of the ActorCritic model.\n",
    "        # The ActorCritic class is initialized with:\n",
    "        # env.action_space().n: The number of possible actions in the environment.\n",
    "        # config[\"ACTIVATION\"]: The type of activation function to use (e.g., ReLU or Tanh).\n",
    "        network_1 = ActorCritic(env.action_space().n, activation=config[\"ACTIVATION\"])\n",
    "        network_2 = ActorCritic(env.action_space().n, activation=config[\"ACTIVATION\"])\n",
    "\n",
    "        # Splits the random number generator rng into two separate RNGs (rng and _rng), so they can be used independently.\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        _rng_1, _rng_2 = jax.random.split(_rng)\n",
    "\n",
    "        # Creates a zero-initialized array of the same shape as the observation space of the environment (env.observation_space().shape), then flattens it into a 1D array (init_x).\n",
    "        # This serves as a sample input to initialize the network.\n",
    "        init_x = jnp.zeros(env.observation_space().shape)\n",
    "        init_x = init_x.flatten()\n",
    "\n",
    "        # Initializes the parameters of the network (network_params) by passing the random key _rng and the input init_x (observation example).\n",
    "        # This is necessary to set up the weights and biases of the neural network layers.\n",
    "        network_1_params = network_1.init(_rng_1, init_x)\n",
    "        network_2_params = network_2.init(_rng_2, init_x)\n",
    "\n",
    "        # This block initializes the optimizer (tx) that will be used to update the network parameters.\n",
    "        if config[\"ANNEAL_LR\"]:\n",
    "            tx = optax.chain(\n",
    "                optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "                optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "            )\n",
    "        else:\n",
    "            tx = optax.chain(optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]), optax.adam(config[\"LR\"], eps=1e-5))\n",
    "\n",
    "        # Creates a TrainState object that holds the model's parameters (network_params), the optimizer (tx), and the function to apply the model (network.apply).\n",
    "        # This will be used for model training, including parameter updates.\n",
    "        train_state_1 = TrainState.create(\n",
    "            apply_fn=network_1.apply,\n",
    "            params=network_1_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        train_state_2 = TrainState.create(\n",
    "            apply_fn=network_2.apply,\n",
    "            params=network_2_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "\n",
    "        # INIT ENV\n",
    "        # Splits the RNG (rng) into two new RNGs: one (_rng) used for resetting the environment, and another (reset_rng) for each environment if you're running multiple environments in parallel (config[\"NUM_ENVS\"]).\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        reset_rng = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "\n",
    "        # Initializes the environment(s). jax.vmap is used to vectorize the env.reset function, so it can reset config[\"NUM_ENVS\"] environments in parallel.\n",
    "        # reset_rng: The random keys for resetting each environment are passed in here.\n",
    "        # obsv: The initial observations returned by the environment(s).\n",
    "        # env_state: The initial state of the environment(s).\n",
    "        obsv, env_state = jax.vmap(env.reset, in_axes=(0,))(reset_rng)\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        @jax.jit\n",
    "        def _update_step(runner_state, unused):\n",
    "            # COLLECT TRAJECTORIES\n",
    "\n",
    "            # Action Selection: It uses the current policy (pi) to sample an action based on the current observations.\n",
    "            # Environment Step: It then steps the environment using the selected actions, receiving observations, rewards, done flags, and additional info.\n",
    "            # Transition Recording: A transition is created, which includes the action taken, value estimate, rewards, and log probabilities for the action. This transition is used to calculate losses later.\n",
    "\n",
    "            def _env_step(runner_state, unused):\n",
    "                train_state_1, train_state_2, env_state, last_obs, rng = runner_state\n",
    "\n",
    "                # SELECT ACTION\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                _rng_1, _rng_2 = jax.random.split(_rng)\n",
    "\n",
    "                # obs_batch = batchify(last_obs, env.agents, config[\"NUM_ACTORS\"])\n",
    "                pi_1, value_1 = network_1.apply(train_state_1.params, jnp.array(last_obs[env.agents[0]]).reshape((int(config[\"NUM_ACTORS\"]/2), -1)))\n",
    "                pi_2, value_2 = network_2.apply(train_state_2.params, jnp.array(last_obs[env.agents[1]]).reshape((int(config[\"NUM_ACTORS\"]/2), -1)))\n",
    "\n",
    "                action_1 = pi_1.sample(seed=_rng_1)\n",
    "                log_prob_1 = pi_1.log_prob(action_1)\n",
    "\n",
    "                action_2 = pi_2.sample(seed=_rng_2)\n",
    "                log_prob_2 = pi_2.log_prob(action_2)\n",
    "\n",
    "                action = jnp.concatenate([action_1, action_2], axis=0)\n",
    "\n",
    "                env_act = unbatchify(action, env.agents, config[\"NUM_ENVS\"], env.num_agents)\n",
    "\n",
    "                env_act = {k:v.flatten() for k,v in env_act.items()}\n",
    "\n",
    "                # STEP ENV\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                rng_step = jax.random.split(_rng, config[\"NUM_ENVS\"])\n",
    "\n",
    "                obsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0,0,0))(\n",
    "                    rng_step, env_state, env_act\n",
    "                )\n",
    "\n",
    "                info_1, info_2 = split_info(info)\n",
    "\n",
    "                transition_1 = Transition(\n",
    "                    jnp.array(done[env.agents[0]]).reshape((int(config[\"NUM_ACTORS\"]/2), -1)).squeeze(),\n",
    "                    action_1,\n",
    "                    value_1,\n",
    "                    jnp.array(reward[env.agents[0]]).reshape((int(config[\"NUM_ACTORS\"]/2), -1)).squeeze(),\n",
    "                    log_prob_1,\n",
    "                    jnp.array(last_obs[env.agents[0]]).reshape((int(config[\"NUM_ACTORS\"]/2), -1)),\n",
    "                    info_1\n",
    "                )\n",
    "                transition_2 = Transition(\n",
    "                    jnp.array(done[env.agents[1]]).reshape((int(config[\"NUM_ACTORS\"]/2), -1)).squeeze(),\n",
    "                    action_2,\n",
    "                    value_2,\n",
    "                    jnp.array(reward[env.agents[1]]).reshape((int(config[\"NUM_ACTORS\"]/2), -1)).squeeze(),\n",
    "                    log_prob_2,\n",
    "                    jnp.array(last_obs[env.agents[1]]).reshape((int(config[\"NUM_ACTORS\"]/2), -1)),\n",
    "                    info_2\n",
    "                )\n",
    "                runner_state = (train_state_1, train_state_2, env_state, obsv, rng)\n",
    "                return runner_state, (transition_1, transition_2)\n",
    "\n",
    "            # This line performs the _env_step function repeatedly for NUM_STEPS. jax.lax.scan is a JAX function that allows you to apply a function repeatedly over a sequence of data, which is used here to simulate multiple environment interactions (collecting trajectories for NUM_STEPS).\n",
    "            runner_state, (traj_batch_1, traj_batch_2) = jax.lax.scan(\n",
    "                _env_step, runner_state, None, config[\"NUM_STEPS\"]\n",
    "            )\n",
    "\n",
    "            # CALCULATE ADVANTAGE\n",
    "            # After collecting the trajectory, this part calculates the advantages of each transition using GAE. The last observation (last_obs) is passed through the network to get the value (last_val) for the last state.\n",
    "            train_state_1, train_state_2, env_state, last_obs, rng = runner_state\n",
    "            # last_obs_batch = batchify(last_obs, env.agents, config[\"NUM_ACTORS\"])\n",
    "\n",
    "            _, last_val_1 = network_1.apply(train_state_1.params, last_obs[env.agents[0]].reshape((int(config[\"NUM_ACTORS\"]/2), -1)))\n",
    "            _, last_val_2 = network_2.apply(train_state_2.params, last_obs[env.agents[1]].reshape((int(config[\"NUM_ACTORS\"]/2), -1)))\n",
    "\n",
    "            # This function implements the GAE algorithm to compute advantages and targets (the value targets).\n",
    "            # delta is computed using the Bellman equation for each transition.\n",
    "            # gae is calculated iteratively in reverse order of the trajectory, allowing for more stable learning by incorporating multiple future rewards.\n",
    "            def _calculate_gae(traj_batch, last_val):\n",
    "                def _get_advantages(gae_and_next_value, transition):\n",
    "                    gae, next_value = gae_and_next_value\n",
    "                    done, value, reward = (\n",
    "                        transition.done,\n",
    "                        transition.value,\n",
    "                        transition.reward,\n",
    "                    )\n",
    "                    delta = reward + config[\"GAMMA\"] * next_value * (1 - done) - value\n",
    "                    gae = (\n",
    "                        delta\n",
    "                        + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "                    )\n",
    "                    return (gae, value), gae\n",
    "\n",
    "                _, advantages = jax.lax.scan(\n",
    "                    _get_advantages,\n",
    "                    (jnp.zeros_like(last_val), last_val),\n",
    "                    traj_batch,\n",
    "                    reverse=True,\n",
    "                    unroll=16,\n",
    "                )\n",
    "                return advantages, advantages + traj_batch.value\n",
    "\n",
    "            advantages_1, targets_1 = _calculate_gae(traj_batch_1, last_val_1)\n",
    "            advantages_2, targets_2 = _calculate_gae(traj_batch_2, last_val_2)\n",
    "\n",
    "            # UPDATE NETWORK 1\n",
    "            def _update_epoch_agent_1(update_state, unused):\n",
    "\n",
    "                # This function updates the model by applying the computed gradients and losses.\n",
    "                # Loss Calculation: The loss function has two parts:\n",
    "                # Value loss: The loss for the value function is the mean squared error between predicted values and the target values (calculated using GAE).\n",
    "                # Actor loss: The loss for the policy (actor) is based on the surrogate objective from the PPO (Proximal Policy Optimization) algorithm, involving the ratio between the current and previous probabilities of actions.\n",
    "                # Entropy loss: A term to encourage exploration by adding entropy to the objective function.\n",
    "                def _update_minbatch(train_state, batch_info):\n",
    "                    traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                    def _loss_fn(params, traj_batch, gae, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        pi, value = network_1.apply(params, traj_batch.obs)\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        value_pred_clipped = traj_batch.value + (\n",
    "                            value - traj_batch.value\n",
    "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = (\n",
    "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "                        )\n",
    "\n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                        loss_actor1 = ratio * gae\n",
    "                        loss_actor2 = (\n",
    "                            jnp.clip(\n",
    "                                ratio,\n",
    "                                1.0 - config[\"CLIP_EPS\"],\n",
    "                                1.0 + config[\"CLIP_EPS\"],\n",
    "                            )\n",
    "                            * gae\n",
    "                        )\n",
    "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                        loss_actor = loss_actor.mean()\n",
    "                        entropy = pi.entropy().mean()\n",
    "\n",
    "                        total_loss = (\n",
    "                            loss_actor\n",
    "                            + config[\"VF_COEF\"] * value_loss\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                    total_loss, grads = grad_fn(\n",
    "                        train_state.params, traj_batch, advantages, targets\n",
    "                    )\n",
    "                    train_state = train_state.apply_gradients(grads=grads)\n",
    "                    return train_state, total_loss\n",
    "\n",
    "                # This code shuffles the data (traj_batch, advantages, targets) into minibatches for efficient training. The training data is reshaped, and the order is randomized to reduce bias during training.\n",
    "                train_state, traj_batch, advantages, targets, rng, agent_num = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
    "                assert (\n",
    "                    batch_size == config[\"NUM_STEPS\"] * int(config[\"NUM_ACTORS\"]/2)\n",
    "                ), \"batch size must be equal to number of steps * number of actors\"\n",
    "                permutation = jax.random.permutation(_rng, batch_size)\n",
    "                batch = (traj_batch, advantages, targets)\n",
    "                batch = jax.tree_util.tree_map(\n",
    "                    lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
    "                )\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "                )\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.reshape(\n",
    "                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "\n",
    "                # This line performs the minibatch updates using the scan function to apply the _update_minbatch function across all minibatches.\n",
    "                train_state, total_loss = jax.lax.scan(\n",
    "                    _update_minbatch, train_state, minibatches\n",
    "                )\n",
    "                update_state = (train_state, traj_batch, advantages, targets, rng, agent_num)\n",
    "                return update_state, total_loss\n",
    "            \n",
    "            # UPDATE NETWORK 1\n",
    "            def _update_epoch_agent_2(update_state, unused):\n",
    "\n",
    "                # This function updates the model by applying the computed gradients and losses.\n",
    "                # Loss Calculation: The loss function has two parts:\n",
    "                # Value loss: The loss for the value function is the mean squared error between predicted values and the target values (calculated using GAE).\n",
    "                # Actor loss: The loss for the policy (actor) is based on the surrogate objective from the PPO (Proximal Policy Optimization) algorithm, involving the ratio between the current and previous probabilities of actions.\n",
    "                # Entropy loss: A term to encourage exploration by adding entropy to the objective function.\n",
    "                def _update_minbatch(train_state, batch_info):\n",
    "                    traj_batch, advantages, targets = batch_info\n",
    "\n",
    "                    def _loss_fn(params, traj_batch, gae, targets):\n",
    "                        # RERUN NETWORK\n",
    "                        pi, value = network_2.apply(params, traj_batch.obs)\n",
    "                        log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                        # CALCULATE VALUE LOSS\n",
    "                        value_pred_clipped = traj_batch.value + (\n",
    "                            value - traj_batch.value\n",
    "                        ).clip(-config[\"CLIP_EPS\"], config[\"CLIP_EPS\"])\n",
    "                        value_losses = jnp.square(value - targets)\n",
    "                        value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                        value_loss = (\n",
    "                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "                        )\n",
    "\n",
    "                        # CALCULATE ACTOR LOSS\n",
    "                        ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                        loss_actor1 = ratio * gae\n",
    "                        loss_actor2 = (\n",
    "                            jnp.clip(\n",
    "                                ratio,\n",
    "                                1.0 - config[\"CLIP_EPS\"],\n",
    "                                1.0 + config[\"CLIP_EPS\"],\n",
    "                            )\n",
    "                            * gae\n",
    "                        )\n",
    "                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                        loss_actor = loss_actor.mean()\n",
    "                        entropy = pi.entropy().mean()\n",
    "\n",
    "                        total_loss = (\n",
    "                            loss_actor\n",
    "                            + config[\"VF_COEF\"] * value_loss\n",
    "                            - config[\"ENT_COEF\"] * entropy\n",
    "                        )\n",
    "                        return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
    "                    total_loss, grads = grad_fn(\n",
    "                        train_state.params, traj_batch, advantages, targets\n",
    "                    )\n",
    "                    train_state = train_state.apply_gradients(grads=grads)\n",
    "                    return train_state, total_loss\n",
    "\n",
    "                # This code shuffles the data (traj_batch, advantages, targets) into minibatches for efficient training. The training data is reshaped, and the order is randomized to reduce bias during training.\n",
    "                train_state, traj_batch, advantages, targets, rng, agent_num = update_state\n",
    "                rng, _rng = jax.random.split(rng)\n",
    "                batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
    "                assert (\n",
    "                    batch_size == config[\"NUM_STEPS\"] * int(config[\"NUM_ACTORS\"]/2)\n",
    "                ), \"batch size must be equal to number of steps * number of actors\"\n",
    "                permutation = jax.random.permutation(_rng, batch_size)\n",
    "                batch = (traj_batch, advantages, targets)\n",
    "                batch = jax.tree_util.tree_map(\n",
    "                    lambda x: x.reshape((batch_size,) + x.shape[2:]), batch\n",
    "                )\n",
    "                shuffled_batch = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "                )\n",
    "                minibatches = jax.tree_util.tree_map(\n",
    "                    lambda x: jnp.reshape(\n",
    "                        x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])\n",
    "                    ),\n",
    "                    shuffled_batch,\n",
    "                )\n",
    "\n",
    "                # This line performs the minibatch updates using the scan function to apply the _update_minbatch function across all minibatches.\n",
    "                train_state, total_loss = jax.lax.scan(\n",
    "                    _update_minbatch, train_state, minibatches\n",
    "                )\n",
    "                update_state = (train_state, traj_batch, advantages, targets, rng, agent_num)\n",
    "                return update_state, total_loss\n",
    "\n",
    "            # This line performs the epoch updates using the scan function to apply the _update_epoch function across all epochs.\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            _rng_1, _rng_2 = jax.random.split(_rng)\n",
    "     \n",
    "            update_state_1 = (train_state_1, traj_batch_1, advantages_1, targets_1, _rng_1, 0)\n",
    "            update_state_2 = (train_state_2, traj_batch_2, advantages_2, targets_2, _rng_2, 1)\n",
    "            update_state_1, loss_info_1 = jax.lax.scan(\n",
    "                _update_epoch_agent_1, update_state_1, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "            update_state_2, loss_info_2 = jax.lax.scan(\n",
    "                _update_epoch_agent_2, update_state_2, None, config[\"UPDATE_EPOCHS\"]\n",
    "            )\n",
    "\n",
    "            # After all epochs of training are completed, the train_state (the updated model) is returned along with metrics (e.g., information about rewards, losses, etc.), and the random number generator (rng) is updated.\n",
    "            train_state_1 = update_state_1[0]\n",
    "            train_state_2 = update_state_2[0]\n",
    "            metric = merge_info(traj_batch_1.info, traj_batch_2.info)\n",
    "            metric = reshape_info(metric) # remove if you want info separated by agent\n",
    "            #rng = update_state_1[-2] # reduces chances of _rng_1 sequence repeating, but not strictly necessary\n",
    "\n",
    "            runner_state = (train_state_1, train_state_2, env_state, last_obs, rng)\n",
    "            return runner_state, metric\n",
    "\n",
    "        # This line splits the rng (random number generator) into two separate random number generators: rng and _rng. This is done so that each part of the code can use a different random stream.\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "\n",
    "        # Here, the runner_state is initialized as a tuple that includes:\n",
    "        # train_state: The current state of the model (including parameters and optimization state).\n",
    "        # env_state: The current state of the environment (e.g., positions, internal states of agents).\n",
    "        # obsv: The current batch of observations (e.g., the states that the agents are observing from the environment).\n",
    "        # _rng: The random number generator that will be used for further random operations in the loop.\n",
    "        runner_state = (train_state_1, train_state_2, env_state, obsv, _rng)\n",
    "\n",
    "        # This line performs an update loop using jax.lax.scan. Here’s how it works:\n",
    "        # jax.lax.scan is a JAX function that allows you to loop over some operation while maintaining the state between iterations, making it ideal for iterative processes like training loops.\n",
    "        # The _update_step function is applied iteratively, where each iteration updates the state of the runner (i.e., the model and environment) and records the metrics (e.g., loss, performance metrics).\n",
    "        # runner_state: This contains all the information required for each update step (including train_state, env_state, etc.).\n",
    "        # None: The second argument is None because the loop doesn’t need any additional data passed each time; it's just evolving the state.\n",
    "        # config[\"NUM_UPDATES\"]: This specifies how many times the _update_step function will be applied. Essentially, this determines how many updates (iterations) will be made to the model.\n",
    "        # runner_state, metric = jax.lax.scan(\n",
    "        #    _update_step, runner_state, None, config[\"NUM_UPDATES\"]\n",
    "        # )\n",
    "\n",
    "        metrics = {}\n",
    "        save_intervals = max(1, config[\"NUM_UPDATES\"] // config[\"NUM_SAVES\"])\n",
    "        save_dir = f'{initial_dir}Seed_{seed}/'\n",
    "\n",
    "        for step in range(config[\"NUM_UPDATES\"]):\n",
    "            runner_state, metric = _update_step(runner_state, step)\n",
    "\n",
    "            if step % save_intervals == 0 or step == config[\"NUM_UPDATES\"] - 1:\n",
    "                save_model(runner_state, save_dir, f\"trained_model_{step}.pkl\")\n",
    "            \n",
    "            returns = metric[\"returned_episode_returns\"]\n",
    "            mean_returns = returns.mean(axis=-1).reshape(-1)\n",
    "\n",
    "            # Save mean_returns in CSV file\n",
    "            with open(csv_file_path, mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([seed, step, mean_returns])\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f\"Seed {seed}, step {step} completed\")\n",
    "\n",
    "        # After executing the loop, runner_state and metric will be updated:\n",
    "        # runner_state: The final state of the model, environment, and random number generator after all the updates.\n",
    "        # metric: A collection of metrics generated during the updates (e.g., losses, rewards, or performance indicators).\n",
    "        return {\"runner_state\": runner_state, \"metrics\": jax.tree.map(lambda x: jnp.stack(x), metrics)}\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one of the classic layouts (cramped_room, asymm_advantages, coord_ring, forced_coord, counter_circuit)\n",
    "# Or make your own! (P=pot, O=onion, A=agent, B=plates, X=deliver)\n",
    "\n",
    "def load_custom_layout(layout_name):\n",
    "    if layout_name == \"custom_1\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WWBWW\n",
    "        WA AW\n",
    "        P   P\n",
    "        W   W\n",
    "        WOXOW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "\n",
    "    elif layout_name == \"custom_2\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WWBWW\n",
    "        WA AW\n",
    "        P   W\n",
    "        P   W\n",
    "        WOXOW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "\n",
    "    elif layout_name == \"custom_3\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WBWWBW\n",
    "        P    P\n",
    "        W AA W\n",
    "        O    O\n",
    "        WXWWXW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "\n",
    "    elif layout_name == \"custom_4\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WBWXW\n",
    "        P W O\n",
    "        WAWAW\n",
    "        P W O\n",
    "        WBWXW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "\n",
    "    elif layout_name == \"custom_5\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WPWPW\n",
    "        W B W\n",
    "        WAWAW\n",
    "        W O W\n",
    "        WXWXW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "\n",
    "    elif layout_name == \"custom_6\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WWBWWWXWW\n",
    "        W   W   W\n",
    "        P   W   O\n",
    "        W   W   W\n",
    "        W A W A W\n",
    "        W   W   W\n",
    "        P   W   O\n",
    "        W   W   W\n",
    "        WWBWWWXWW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "\n",
    "    elif layout_name == \"custom_7\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WPWPW\n",
    "        W B B\n",
    "        WAWAW\n",
    "        W O O\n",
    "        WXWXW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "    \n",
    "    elif layout_name == \"custom_8\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WBWWW\n",
    "        P W O\n",
    "        W W W\n",
    "        PAWAO\n",
    "        W W W\n",
    "        P W O\n",
    "        WXWWW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "\n",
    "    elif layout_name == \"custom_9\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WBWBW\n",
    "        P W O\n",
    "        W W P\n",
    "        WAWAO\n",
    "        W W P\n",
    "        P W O\n",
    "        WXWBW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "\n",
    "    elif layout_name == \"custom_10\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WWPWPWW\n",
    "        W     W\n",
    "        B WWW X\n",
    "        WA   AW\n",
    "        WWOWOWW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "\n",
    "    elif layout_name == \"custom_11\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WWWWW\n",
    "        WAWAP\n",
    "        W W O\n",
    "        WXWBW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "\n",
    "    elif layout_name == \"custom_12\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WWPWW\n",
    "        W   W\n",
    "        B W X\n",
    "        WA AW\n",
    "        WWOWW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "\n",
    "    elif layout_name == \"custom_13\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WWBWW\n",
    "        W   W\n",
    "        P W X\n",
    "        WA AW\n",
    "        WWOWW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "\n",
    "    elif layout_name == \"custom_14\":\n",
    "        custom_layout_grid = dedent(\n",
    "        \"\"\"\n",
    "        WPWPW\n",
    "        WABAW\n",
    "        W O W\n",
    "        WXWXW\n",
    "        \"\"\"\n",
    "        ).strip()\n",
    "    \n",
    "    custom_layout = layout_grid_to_dict(custom_layout_grid)\n",
    "    return custom_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration saved in /mnt/lustre/home/samuloza/data/samuel_lozano/hfsp_collective_learning/data_JaxMARL/Asymmetric_Agents/custom_5/Checkpoints_2025-03-28_16-39-15/configuration_custom_5_2025-03-28_16-39-15.txt\n",
      "Model parameters saved to /mnt/lustre/home/samuloza/data/samuel_lozano/hfsp_collective_learning/data_JaxMARL/Asymmetric_Agents/custom_5/Checkpoints_2025-03-28_16-39-15/Seed_0/trained_model_0.pkl\n",
      "Seed 0, step 0 completed\n",
      "Model parameters saved to /mnt/lustre/home/samuloza/data/samuel_lozano/hfsp_collective_learning/data_JaxMARL/Asymmetric_Agents/custom_5/Checkpoints_2025-03-28_16-39-15/Seed_0/trained_model_52.pkl\n",
      "Seed 0, step 100 completed\n",
      "Model parameters saved to /mnt/lustre/home/samuloza/data/samuel_lozano/hfsp_collective_learning/data_JaxMARL/Asymmetric_Agents/custom_5/Checkpoints_2025-03-28_16-39-15/Seed_0/trained_model_104.pkl\n"
     ]
    }
   ],
   "source": [
    "cluster = \"brigit\"\n",
    "layout_name = \"custom_5\"\n",
    "\n",
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "if cluster == \"cuenca\":\n",
    "    initial_dir = f'/data/samuel_lozano/hfsp_collective_learning/data_overcooked_original/Asymmetric_Agents/{layout_name}/Checkpoints_{current_datetime}/'\n",
    "elif cluster == \"brigit\":\n",
    "    initial_dir = f'/mnt/lustre/home/samuloza/data/samuel_lozano/hfsp_collective_learning/data_overcooked_original/Asymmetric_Agents/{layout_name}/Checkpoints_{current_datetime}/'\n",
    "else:\n",
    "    print(\"Introduce a valid cluster name\")\n",
    "\n",
    "# Save the original stdout reference (ONLY NOTEBOOKS)\n",
    "original_stdout = sys.stdout  \n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(initial_dir, exist_ok=True)\n",
    "\n",
    "# Define the output file name\n",
    "log_filename = f\"configuration_{layout_name}_{current_datetime}.txt\"\n",
    "log_filepath = os.path.join(initial_dir, log_filename)\n",
    "\n",
    "# Redirect standard output to a file\n",
    "with open(log_filepath, \"w\") as log_file:\n",
    "    sys.stdout = log_file  # Redirect stdout to the file\n",
    "\n",
    "    print(f\"Layout: {layout_name}\")\n",
    "    print(f\"Timestamp: {current_datetime}\\n\")\n",
    "    custom_layout = load_custom_layout(layout_name)\n",
    "\n",
    "    # set hyperparameters:\n",
    "    config = {\n",
    "        # Number of possible actions in the environment. \n",
    "        \"NUM_ACTIONS\": 6, \n",
    "        # Controls how much the model updates its weights during optimization.\n",
    "        \"LR\": 1e-4, \n",
    "        # Number of parallel environments running simultaneously.\n",
    "        \"NUM_ENVS\": 8, \n",
    "        #Number of steps collected before running an update.\n",
    "        \"NUM_STEPS\": 1600, \n",
    "        # Total number of timesteps for training.\n",
    "        \"TOTAL_TIMESTEPS\": 1e7, \n",
    "        # Number of times each collected batch of experiences is used for gradient updates. More epochs allow for better learning from collected data but can lead to overfitting.\n",
    "        \"UPDATE_EPOCHS\": 4, \n",
    "        # Number of minibatches used during training updates. More minibatches reduce variance but increase computational cost.\n",
    "        \"NUM_MINIBATCHES\": 4, \n",
    "        # Discount factor for future rewards. A value close to 1 favors long-term rewards, while lower values prioritize immediate rewards.\n",
    "        \"GAMMA\": 0.99, \n",
    "        # Controls Generalized Advantage Estimation (GAE). Higher values lead to smoother, less biased advantage estimates.\n",
    "        \"GAE_LAMBDA\": 0.99, \n",
    "        # PPO-specific parameter that limits policy updates to prevent excessive changes. Lower values ensure stability but may slow training.\n",
    "        \"CLIP_EPS\": 0.2, \n",
    "        # Coefficient for entropy regularization, encouraging exploration. Higher values lead to more randomness in actions.\n",
    "        \"ENT_COEF\": 0.1, \n",
    "        # Coefficient for the value function loss. Higher values prioritize accurate value function learning.\n",
    "        \"VF_COEF\": 0.5, \n",
    "        # Limits the magnitude of gradients, preventing instability.\n",
    "        \"MAX_GRAD_NORM\": 0.5, \n",
    "        # Specifies the activation function for the network. Tanh helps with stable gradient flow but might limit expressiveness compared to ReLU.\n",
    "        \"ACTIVATION\": \"tanh\",\n",
    "        # The RL environment being used.\n",
    "        \"ENV_NAME\": \"overcooked\",\n",
    "        # Allows customization of environment settings, such as custom layouts.\n",
    "        \"ENV_KWARGS\": {\n",
    "          \"layout\" : custom_layout #write custom_layout without quotation marks if you want to use the custom layout\n",
    "        },\n",
    "        # If enabled, the learning rate decreases over time, improving stability in later training stages.\n",
    "        \"ANNEAL_LR\": True, \n",
    "        # Ensures reproducibility by fixing the random seed.\n",
    "        \"SEED\": 0,\n",
    "        # Runs multiple training instances with different seeds for robustness.\n",
    "        \"NUM_SEEDS\": 3,\n",
    "        # Saves model checkpoints every N epochs.\n",
    "        \"SAVE_EVERY_N_EPOCHS\": 50,\n",
    "    }\n",
    "\n",
    "    #Comment the following line if you want to use the custom_layout\n",
    "    #config[\"ENV_KWARGS\"][\"layout\"] = overcooked_layouts[config[\"ENV_KWARGS\"][\"layout\"]]\n",
    "\n",
    "    config = make_config(config)\n",
    "\n",
    "    log_file.flush()\n",
    "\n",
    "# Restore standard output to the console\n",
    "sys.stdout = original_stdout\n",
    "\n",
    "print(f\"Configuration saved in {log_filepath}\")\n",
    "\n",
    "rng = jax.random.PRNGKey(config[\"SEED\"])\n",
    "rngs = jax.random.split(rng, config[\"NUM_SEEDS\"])\n",
    "csv_file_path = f'{initial_dir}mean_return_data_{layout_name}_{current_datetime}.csv'\n",
    "os.makedirs(initial_dir, exist_ok=True)\n",
    "\n",
    "with open(csv_file_path, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"seed\", \"step\", \"mean_returns\"])\n",
    "\n",
    "# Training loop\n",
    "for seed in range(config[\"NUM_SEEDS\"]):\n",
    "    train_jit = make_train(config)\n",
    "    result = train_jit(seed, rngs[seed], initial_dir, csv_file_path)\n",
    "    print(f\"Data saved in {initial_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RECREATING RESULTS OF TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD (WITHOUT CSV, WITH NPZ)\n",
    "\n",
    "def recreate_results(load_layout_name, load_datetime, show_figure=1):\n",
    "    original_dir = f'/data/samuel_lozano/hfsp_collective_learning/data_overcooked_original/Asymmetric_Agents/{load_layout_name}/Checkpoints_{load_datetime}/'\n",
    "    # Find all files matching the pattern \"plot_data_seed_{seed}.npz\"\n",
    "    seed_files = [f for f in os.listdir(original_dir) if re.match(r\"plot_data_seed_\\d+\\.npz\", f)]\n",
    "    seed_files.sort(key=lambda x: int(re.findall(r\"\\d+\", x)[0]))  # Sort by seed number\n",
    "\n",
    "    if not seed_files:\n",
    "        print(\"No seed files found!\")\n",
    "        return None, None, None\n",
    "    \n",
    "    max_steps_per_seed = {}\n",
    "    mean_returns_per_seed = [] \n",
    "\n",
    "    i = 0\n",
    "    for file in seed_files:\n",
    "        file_path = os.path.join(original_dir, file)\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        returns_list = jnp.expand_dims(data[\"returns\"], axis=0)\n",
    "        out_reconstructed = [{\"metrics\": {\"returned_episode_returns\": returns}} for returns in returns_list]\n",
    "    \n",
    "        returns = jnp.stack(out_reconstructed[0][\"metrics\"][\"returned_episode_returns\"])\n",
    "        mean_returns = returns.mean(axis=-1).reshape(-1)\n",
    "\n",
    "        mean_returns_per_seed.append(mean_returns)\n",
    "        \n",
    "        max_index = int(jnp.argmax(mean_returns))  # Index of max return\n",
    "        max_step = max_index / config[\"NUM_STEPS\"]  # Convert to update steps\n",
    "        max_value = float(mean_returns[max_index])  # Get max value\n",
    "        \n",
    "        max_steps_per_seed[f\"Seed {i}\"] = (max_step, max_value)\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    # Recreate the plot\n",
    "    if show_figure == 1:\n",
    "        plt.figure()\n",
    "        for i in range(len(seed_files)):\n",
    "            plt.plot(mean_returns_per_seed[i], label=f\"Seed {i}\")\n",
    "    \n",
    "        plt.xlabel(\"Update Step\")\n",
    "        plt.ylabel(\"Return\")\n",
    "        plt.legend()\n",
    "    \n",
    "        save_path = os.path.join(original_dir, f\"plot_{load_layout_name}_{load_datetime}.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.show()\n",
    "\n",
    "    print(\"Max steps per seed (reconstructed):\")\n",
    "    print(max_steps_per_seed)\n",
    "    return mean_returns_per_seed, max_steps_per_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={}\n",
    "config[\"NUM_STEPS\"] = 1000\n",
    "\n",
    "# Directory containing the seed files\n",
    "load_datetime = '2025-03-22_14-11-41'\n",
    "load_layout_name = 'custom_1'\n",
    "\n",
    "# Run the function\n",
    "show_figure = 1\n",
    "mean_returns_per_seed, max_steps_per_seed = recreate_results(load_layout_name, load_datetime, show_figure=show_figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_results_csv(cluster, load_layout_name, load_datetime, show_figure=1):\n",
    "    if cluster == \"cuenca\":\n",
    "        original_dir = f'/data/samuel_lozano/hfsp_collective_learning/data_overcooked_original/Asymmetric_Agents/{load_layout_name}/Checkpoints_{load_datetime}/'\n",
    "    elif cluster == \"brigit\":\n",
    "        original_dir = f'/mnt/lustre/home/samuloza/data/samuel_lozano/hfsp_collective_learning/data_overcooked_original/Asymmetric_Agents/{load_layout_name}/Checkpoints_{load_datetime}/'\n",
    "    else:\n",
    "        print(\"Introduce a valid cluster name\")\n",
    "        return\n",
    "\n",
    "    data = pd.read_csv(f'{original_dir}mean_return_data_{load_layout_name}_{load_datetime}.csv')\n",
    "    max_steps_per_seed = {}\n",
    "    mean_returns_per_seed = []\n",
    "\n",
    "    for seed in data['seed'].unique():\n",
    "        seed_data = data[data['seed'] == seed]\n",
    "        seed_data = seed_data.sort_values('step')\n",
    "        \n",
    "        mean_returns = seed_data['mean_returns'].apply(lambda x: float(x.strip('[]'))).to_numpy()\n",
    "        mean_returns_per_seed.append(mean_returns)\n",
    "\n",
    "        max_index = int(jnp.argmax(mean_returns))  # Index of max return\n",
    "        max_value = float(mean_returns[max_index])  # Get max value\n",
    "        \n",
    "        max_steps_per_seed[f\"Seed {seed}\"] = (max_index, max_value)\n",
    "    \n",
    "    if show_figure == 1:\n",
    "        plt.figure()\n",
    "        for seed_index in range(len(mean_returns_per_seed)):\n",
    "            steps = data[data['seed'] == seed_index]['step'].unique()  \n",
    "            plt.plot(steps, mean_returns_per_seed[seed_index], label=f'Seed {seed_index}')\n",
    "    \n",
    "        plt.xlabel('Update Step')\n",
    "        plt.ylabel('Mean Returns')\n",
    "        plt.ylim(-10, 175)\n",
    "        plt.legend() \n",
    "        \n",
    "        save_path = os.path.join(original_dir, f\"plot_{load_layout_name}_{load_datetime}.png\")\n",
    "        plt.savefig(save_path)\n",
    "        plt.show()\n",
    "\n",
    "    print(\"Max steps per seed (reconstructed):\")\n",
    "    print(max_steps_per_seed)\n",
    "    return mean_returns_per_seed, max_steps_per_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max steps per seed (reconstructed):\n",
      "{'Seed 0': (0, 0.0), 'Seed 1': (0, 0.0), 'Seed 2': (0, 0.0)}\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the seed files\n",
    "cluster = \"brigit\"\n",
    "load_datetime = '2025-04-03_08-02-47'\n",
    "load_layout_name = 'custom_13'\n",
    "\n",
    "# Run the function\n",
    "show_figure = 1\n",
    "mean_returns_per_seed, max_steps_per_seed = recreate_results_csv(cluster, load_layout_name, load_datetime, show_figure=show_figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = int(3.15e3)\n",
    "\n",
    "plt.figure() \n",
    "for i in range(len(mean_returns_per_seed)):    \n",
    "    plt.plot(mean_returns_per_seed[i][:max_steps], label=f\"Seed {i}\")\n",
    "\n",
    "plt.xlabel(\"Update Step\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.ylim(-10, 195)\n",
    "plt.legend()\n",
    "\n",
    "if cluster == \"cuenca\":\n",
    "    original_dir = f'/data/samuel_lozano/hfsp_collective_learning/data_overcooked_original/Asymmetric_Agents/{load_layout_name}/Checkpoints_{load_datetime}/'\n",
    "elif cluster == \"brigit\":\n",
    "    original_dir = f'/mnt/lustre/home/samuloza/data/samuel_lozano/hfsp_collective_learning/data_overcooked_original/Asymmetric_Agents/{load_layout_name}/Checkpoints_{load_datetime}/'\n",
    "else:\n",
    "    print(\"Introduce a valid cluster name\")\n",
    "\n",
    "save_path = os.path.join(original_dir, f\"plot_zoom_{load_layout_name}_{load_datetime}.png\")\n",
    "plt.savefig(save_path)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_value_return_interval(seed, mean_returns_per_seed, config, searched_value, min_step, max_step):\n",
    "    for step in range(int(min_step), int(max_step + 1)):\n",
    "        scaled_step = step // config[\"NUM_STEPS\"]  # Adjust step according to NUM_STEPS\n",
    "        if mean_returns_per_seed[seed][step] == searched_value:\n",
    "            print(f\"First return value {searched_value} in seed {seed} at update_step: {step}. In update step: {scaled_step}\")\n",
    "            return scaled_step\n",
    "\n",
    "    print(f\"No return value {searched_value} found in seed {seed} within the specified range.\")\n",
    "    return None\n",
    "\n",
    "def find_different_value_return_interval(seed, mean_returns_per_seed, config, searched_value, min_step, max_step):\n",
    "    for step in range(int(min_step), int(max_step + 1)):\n",
    "        scaled_step = step // config[\"NUM_STEPS\"]  # Adjust step according to NUM_STEPS\n",
    "        if mean_returns_per_seed[seed][step] != searched_value:\n",
    "            print(f\"First return value {searched_value} in seed {seed} at update_step: {step}. In update step: {scaled_step}\")\n",
    "            return scaled_step\n",
    "\n",
    "    print(f\"No return value {searched_value} found in seed {seed} within the specified range.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2\n",
    "searched_value = 0\n",
    "min_step = 4e6\n",
    "max_step = 4.3e6\n",
    "\n",
    "found_step = find_value_return_interval(seed, mean_returns_per_seed, config, searched_value, min_step, max_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_steps(load_layout_name, load_datetime, mean_returns_per_seed, min_value, max_value, seed=None):\n",
    "    if cluster == \"cuenca\":\n",
    "        original_dir = f'/data/samuel_lozano/hfsp_collective_learning/data_overcooked_original/Asymmetric_Agents/{load_layout_name}/Checkpoints_{load_datetime}/'\n",
    "    elif cluster == \"brigit\":\n",
    "        original_dir = f'/mnt/lustre/home/samuloza/data/samuel_lozano/hfsp_collective_learning/data_overcooked_original/Asymmetric_Agents/{load_layout_name}/Checkpoints_{load_datetime}/'\n",
    "    else:\n",
    "        print(\"Introduce a valid cluster name\")\n",
    "        return\n",
    "\n",
    "    step_pattern = re.compile(r\"trained_model_(\\d+)\\.pkl\")\n",
    "\n",
    "    results = {}  # Final dictionary with the reconstructed values\n",
    "\n",
    "    seed_path = os.path.join(original_dir, f\"Seed_{seed}\")   \n",
    "    if os.path.isdir(seed_path):\n",
    "        steps = []\n",
    "        for file in os.listdir(seed_path):\n",
    "            step_match = step_pattern.match(file)\n",
    "            if step_match and min_value <= int(step_match.group(1)) and max_value >= int(step_match.group(1)):\n",
    "                steps.append(int(step_match.group(1)))\n",
    "\n",
    "        steps.sort()\n",
    "        for step in steps:                 \n",
    "            results[step] = mean_returns_per_seed[seed][step]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.0, 50: 0.0, 100: 0.0, 150: 0.0, 200: 0.0, 250: 0.0, 300: 0.0, 350: 0.0, 400: 0.0, 450: 0.0, 500: 0.0, 550: 0.0, 600: 0.0, 650: 0.0, 700: 0.0, 750: 0.0, 800: 0.0, 850: 0.0, 900: 0.0, 950: 0.0, 1000: 0.0, 1050: 0.0, 1100: 0.0, 1150: 0.0, 1200: 0.0, 1250: 0.0, 1300: 0.0, 1350: 0.0, 1400: 0.0, 1450: 0.0, 1500: 0.0, 1550: 0.0, 1600: 0.0, 1650: 0.0, 1700: 0.0, 1750: 0.0, 1800: 0.0, 1850: 0.0, 1900: 0.0, 1950: 0.0, 2000: 0.0, 2050: 0.0, 2100: 0.0, 2150: 0.0, 2200: 0.0, 2250: 0.0, 2300: 0.0, 2350: 0.0, 2400: 0.0, 2450: 0.0, 2500: 0.0, 2550: 0.0, 2600: 0.0, 2650: 0.0, 2700: 0.0, 2750: 0.0, 2800: 0.0, 2850: 0.0, 2900: 0.0, 2950: 0.0, 3000: 0.0, 3050: 0.0, 3100: 0.0, 3150: 0.0, 3200: 0.0, 3250: 0.0, 3300: 0.0, 3350: 0.0, 3400: 0.0, 3450: 0.0, 3500: 0.0, 3550: 0.0, 3600: 0.0, 3650: 0.0, 3700: 0.0, 3750: 0.0, 3800: 0.0, 3850: 0.0, 3900: 0.0, 3950: 0.0, 4000: 0.0, 4050: 0.0, 4100: 0.0, 4150: 0.0, 4200: 0.0, 4250: 0.0, 4300: 0.0, 4350: 0.0, 4400: 0.0, 4450: 0.0, 4500: 0.0, 4550: 0.0, 4600: 0.0, 4650: 0.0, 4700: 0.0, 4750: 0.0, 4800: 0.0, 4850: 0.0, 4900: 0.0, 4950: 0.0, 5000: 0.0, 5050: 0.0, 5100: 0.0, 5150: 0.0, 5200: 0.0, 5250: 0.0, 5300: 0.0, 5350: 0.0, 5400: 0.0, 5450: 0.0, 5500: 0.0, 5550: 0.0, 5600: 0.0, 5650: 0.0, 5700: 0.0, 5750: 0.0, 5800: 0.0, 5850: 0.0, 5900: 0.0, 5950: 0.0, 6000: 0.0, 6050: 0.0, 6100: 0.0, 6150: 0.0, 6200: 0.0, 6250: 0.0, 6300: 0.0, 6350: 0.0, 6400: 0.0, 6450: 0.0, 6500: 0.0, 6550: 0.0, 6600: 0.0, 6650: 0.0, 6700: 0.0, 6750: 0.0, 6800: 0.0, 6850: 0.0, 6900: 0.0, 6950: 0.0, 7000: 0.0, 7050: 0.0, 7100: 0.0, 7150: 0.0, 7200: 0.0, 7250: 0.0, 7300: 0.0, 7350: 0.0, 7400: 0.0, 7450: 0.0, 7500: 0.0, 7550: 0.0, 7600: 0.0, 7650: 0.0, 7700: 0.0, 7750: 0.0, 7800: 0.0, 7850: 0.0, 7900: 0.0, 7950: 0.0, 8000: 0.0, 8050: 0.0, 8100: 0.0, 8150: 0.0, 8200: 0.0, 8250: 0.0, 8300: 0.0, 8350: 0.0, 8400: 0.0, 8450: 0.0, 8500: 0.0, 8550: 0.0, 8600: 0.0, 8650: 0.0, 8700: 0.0, 8750: 0.0, 8800: 0.0, 8850: 0.0, 8900: 0.0, 8950: 0.0, 9000: 0.0, 9050: 0.0, 9100: 0.0, 9150: 0.0, 9200: 0.0, 9250: 0.0, 9300: 0.0, 9350: 0.0, 9400: 0.0, 9450: 0.0, 9500: 0.0, 9550: 0.0, 9600: 0.0, 9650: 0.0, 9700: 0.0, 9750: 0.0, 9800: 0.0, 9850: 0.0, 9900: 0.0, 9950: 0.0, 10000: 0.0, 10050: 0.0, 10100: 0.0, 10150: 0.0, 10200: 0.0, 10250: 0.0, 10300: 0.0, 10350: 0.0, 10400: 0.0, 10450: 0.0, 10500: 0.0, 10550: 0.0, 10600: 0.0, 10650: 0.0, 10700: 0.0, 10750: 0.0, 10800: 0.0, 10850: 0.0, 10900: 0.0, 10950: 0.0, 11000: 0.0, 11050: 0.0, 11100: 0.0, 11150: 0.0, 11200: 0.0, 11250: 0.0, 11300: 0.0, 11350: 0.0, 11400: 0.0, 11450: 0.0, 11500: 0.0, 11550: 0.0, 11600: 0.0, 11650: 0.0, 11700: 0.0, 11750: 0.0, 11800: 0.0, 11850: 0.0, 11900: 0.0, 11950: 0.0, 12000: 0.0, 12050: 0.0, 12100: 0.0, 12150: 0.0, 12200: 0.0, 12250: 0.0, 12300: 0.0, 12350: 0.0, 12400: 0.0, 12450: 0.0, 12500: 0.0, 12550: 0.0, 12600: 0.0, 12650: 0.0, 12700: 0.0, 12750: 0.0, 12800: 0.0, 12850: 0.0, 12900: 0.0, 12950: 0.0, 13000: 0.0, 13050: 0.0, 13100: 0.0, 13150: 0.0, 13200: 0.0, 13250: 0.0, 13300: 0.0, 13350: 0.0, 13400: 0.0, 13450: 0.0, 13500: 0.0, 13550: 0.0, 13600: 0.0, 13650: 0.0, 13700: 0.0, 13750: 0.0, 13800: 0.0, 13850: 0.0, 13900: 0.0, 13950: 0.0, 14000: 0.0, 14050: 0.0, 14100: 0.0, 14150: 0.0, 14200: 0.0, 14250: 0.0, 14300: 0.0, 14350: 0.5, 14400: 0.5, 14450: 0.0, 14500: 1.50125, 14550: 7.5050006, 14600: 32.998753, 14650: 69.00125, 14700: 82.99876, 14750: 89.50001, 14800: 90.50125, 14850: 94.00125, 14900: 97.99876, 14950: 97.00001, 15000: 93.005005, 15050: 96.50001, 15100: 94.00001, 15150: 96.50125, 15200: 95.50125, 15250: 96.00125, 15300: 95.0025, 15350: 97.497505, 15400: 98.0025, 15450: 100.497505, 15500: 96.99876, 15550: 102.997505, 15600: 100.505005, 15650: 103.50375, 15700: 104.00001, 15750: 106.496254, 15800: 105.5025, 15850: 106.50001, 15900: 104.50125, 15950: 103.49876, 16000: 108.50126, 16050: 108.0025, 16100: 110.50001, 16150: 109.505005, 16200: 110.497505, 16250: 110.00001, 16300: 109.50126, 16350: 109.5025, 16400: 107.00001, 16450: 109.00126, 16500: 110.00001, 16550: 111.49876, 16600: 111.00375, 16650: 106.00751, 16700: 109.49876, 16750: 110.00126, 16800: 112.00001, 16850: 114.49876, 16900: 110.00375, 16950: 115.0025, 17000: 112.00126, 17050: 112.497505, 17100: 114.0025, 17150: 116.50001, 17200: 115.00126, 17250: 111.00001, 17300: 113.497505, 17350: 116.997505, 17400: 110.50001, 17450: 113.50126, 17500: 115.505005, 17550: 113.00126, 17600: 115.496254, 17650: 111.00126, 17700: 115.00126, 17750: 112.506256, 17800: 115.005005, 17850: 114.0025, 17900: 117.00001, 17950: 113.50001, 18000: 115.50001, 18050: 117.495, 18100: 113.99876, 18150: 116.997505, 18200: 115.0025, 18250: 115.00001, 18300: 116.50126, 18350: 118.00126, 18400: 119.50001, 18450: 116.0025, 18500: 114.50375, 18550: 116.99876, 18600: 118.49876, 18650: 119.50001, 18700: 119.49876, 18750: 115.49876, 18800: 117.00126, 18850: 120.50001, 18900: 119.997505, 18950: 114.99876, 19000: 121.00001, 19050: 118.49876, 19100: 115.00126, 19150: 121.00001, 19200: 119.997505, 19250: 118.0025, 19300: 120.00126, 19350: 119.00126, 19400: 120.997505, 19450: 119.50375, 19500: 119.50126, 19550: 119.49876, 19600: 121.00001, 19650: 117.00126, 19700: 118.505005, 19750: 117.00375, 19800: 120.00001, 19850: 124.00375, 19900: 121.00126, 19950: 122.497505, 20000: 119.50001, 20050: 120.00001, 20100: 122.496254, 20150: 118.5025, 20200: 118.997505, 20250: 119.0025, 20300: 118.50126, 20350: 121.497505, 20400: 123.99251, 20450: 118.0025, 20500: 119.996254, 20550: 115.50126, 20600: 117.49876, 20650: 120.99876, 20700: 123.49876, 20750: 120.00126, 20800: 120.997505, 20850: 119.497505, 20900: 117.0025, 20950: 119.50126, 21000: 120.49876, 21050: 122.00001, 21100: 118.50001, 21150: 121.00001, 21200: 121.00126, 21250: 119.49876, 21300: 117.0025, 21350: 123.50126, 21400: 124.50001, 21450: 122.00001, 21500: 119.50001, 21550: 117.997505, 21600: 119.496254, 21650: 120.0025, 21700: 124.0025, 21750: 120.49876, 21800: 122.00001, 21850: 121.50126, 21900: 121.00001, 21950: 121.49876, 22000: 124.49876, 22050: 119.99876, 22100: 120.99876, 22150: 121.50126, 22200: 121.50001, 22250: 125.5025, 22300: 122.00001, 22350: 124.997505, 22400: 121.00001, 22450: 121.00001, 22500: 121.00001, 22550: 120.00126, 22600: 126.49876, 22650: 121.50001, 22700: 122.49876, 22750: 121.996254, 22800: 125.0025, 22850: 122.496254, 22900: 122.49876, 22950: 123.00001, 23000: 124.00126, 23050: 119.99876, 23100: 122.49876, 23150: 119.997505, 23200: 124.997505, 23250: 120.50001, 23300: 120.49876, 23350: 120.005005, 23400: 123.00375, 23450: 122.497505, 23500: 122.0025, 23550: 121.00001, 23600: 120.5025, 23650: 122.00001, 23700: 120.99876, 23750: 123.0025, 23800: 122.506256, 23850: 120.00001, 23900: 124.997505, 23950: 121.50126, 24000: 124.00126, 24050: 120.49876, 24100: 122.5025, 24150: 121.99376, 24200: 121.00375, 24250: 123.00001, 24300: 122.5025, 24350: 121.50001, 24400: 119.997505, 24450: 119.50126, 24500: 121.50126, 24550: 124.50001, 24600: 123.50126, 24650: 121.5025, 24700: 124.49876, 24750: 122.49876, 24800: 121.996254, 24850: 123.997505, 24900: 123.0025, 24950: 123.0025, 25000: 123.49876, 25050: 125.00001, 25100: 123.00126, 25150: 124.0025, 25200: 121.50001, 25250: 123.0025, 25300: 121.00001, 25350: 123.49876, 25400: 123.50375, 25450: 123.50001, 25500: 120.997505, 25550: 120.99876, 25600: 122.0025, 25650: 122.50001, 25700: 123.50126, 25750: 124.00001, 25800: 124.99876, 25850: 122.0025, 25900: 120.50001, 25950: 123.99876, 26000: 123.99876, 26050: 122.99876, 26100: 122.00001, 26150: 122.497505, 26200: 128.5, 26250: 123.50001, 26300: 124.49876, 26350: 122.997505, 26400: 123.006256, 26450: 122.00001, 26500: 122.50126, 26550: 123.50001, 26600: 123.99876, 26650: 122.99876, 26700: 120.50001, 26750: 125.00126, 26800: 121.0025, 26850: 122.496254, 26900: 119.00001, 26950: 122.496254, 27000: 125.00001, 27050: 124.996254, 27100: 121.5025, 27150: 125.00126, 27200: 123.00126, 27250: 120.50126, 27300: 127.99876, 27350: 121.50126, 27400: 124.00126, 27450: 120.997505, 27500: 122.49876, 27550: 124.50001, 27600: 122.00001, 27650: 122.99876, 27700: 124.00001, 27750: 124.00001, 27800: 126.00001, 27850: 120.5025, 27900: 123.996254, 27950: 123.497505, 28000: 121.00001, 28050: 121.506256, 28100: 124.50001, 28150: 121.497505, 28200: 124.496254, 28250: 122.50126, 28300: 125.00126, 28350: 125.496254, 28400: 124.00126, 28450: 122.50375, 28500: 123.497505, 28550: 124.00126, 28600: 123.00001, 28650: 123.50126, 28700: 122.0025, 28750: 121.50001, 28800: 120.99876, 28850: 126.996254, 28900: 124.497505, 28950: 123.997505, 29000: 125.99876, 29050: 123.497505, 29100: 120.49876, 29150: 124.99876, 29200: 122.996254, 29250: 124.997505, 29300: 123.99876, 29350: 123.00001, 29400: 123.00375, 29450: 123.00375, 29500: 122.50001, 29550: 123.00126, 29600: 125.505005, 29650: 124.505005, 29700: 120.0025, 29750: 124.50126, 29800: 122.00126, 29850: 124.00001, 29900: 122.49876, 29950: 122.50126, 30000: 122.49876, 30050: 125.50126, 30100: 122.00001, 30150: 122.5025, 30200: 123.497505, 30250: 122.99876, 30300: 122.00001, 30350: 122.00126, 30400: 124.00126, 30450: 122.50126, 30500: 122.00001, 30550: 123.0025, 30600: 123.00001, 30650: 121.997505, 30700: 122.50375, 30750: 124.99876, 30800: 124.496254, 30850: 124.50001, 30900: 123.005005, 30950: 124.50001, 31000: 122.0025, 31050: 122.50375, 31100: 124.99876, 31150: 124.49876, 31200: 125.00001, 31249: 123.496254}\n",
      "Key with the highest value: 26200, Value: 128.5\n"
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "min_value = int(0e0)\n",
    "max_value = int(4e4)\n",
    "\n",
    "steps_per_seed = get_trained_steps(load_layout_name, load_datetime, mean_returns_per_seed, min_value, max_value, seed)\n",
    "print(steps_per_seed)\n",
    "\n",
    "# Get the key with the maximum value\n",
    "max_key = max(steps_per_seed, key=steps_per_seed.get)\n",
    "\n",
    "# Print the key and its corresponding value\n",
    "print(f\"Key with the highest value: {max_key}, Value: {steps_per_seed[max_key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADING AND EVALUATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found checkpoint: trained_model_26200.pkl for step 26200\n",
      "Model parameters loaded from /mnt/lustre/home/samuloza/data/samuel_lozano/hfsp_collective_learning/data_overcooked_original/Asymmetric_Agents/custom_13/Checkpoints_2025-04-03_01-04-00/Seed_0/trained_model_26200.pkl\n",
      "Model restored successfully from step 26200\n",
      "with mean return 128.5!\n"
     ]
    }
   ],
   "source": [
    "fixed_step = 26200  # Change this to the desired step\n",
    "\n",
    "seed_idx = 0  # Change if needed\n",
    "\n",
    "#cluster = \"brigit\"\n",
    "#load_datetime = '2025-03-26_14-56-22'\n",
    "#load_layout_name = 'custom_8'\n",
    "custom_layout = load_custom_layout(load_layout_name)\n",
    "\n",
    "if cluster == \"cuenca\":\n",
    "    original_dir = f'/data/samuel_lozano/hfsp_collective_learning/data_overcooked_original/Asymmetric_Agents/{load_layout_name}/Checkpoints_{load_datetime}/'\n",
    "elif cluster == \"brigit\":\n",
    "    original_dir = f'/mnt/lustre/home/samuloza/data/samuel_lozano/hfsp_collective_learning/data_overcooked_original/Asymmetric_Agents/{load_layout_name}/Checkpoints_{load_datetime}/'\n",
    "else:\n",
    "    print(\"Introduce a valid cluster name\")\n",
    "\n",
    "load_dir = f\"{original_dir}Seed_{seed_idx}/\"\n",
    "load_filename = f\"trained_model_{fixed_step}.pkl\"\n",
    "\n",
    "num_actions = 6\n",
    "activation = \"tanh\"\n",
    "\n",
    "# Initialize network\n",
    "network_1 = ActorCritic(num_actions, activation)\n",
    "network_2 = ActorCritic(num_actions, activation)\n",
    "\n",
    "# Load parameters into the network\n",
    "model_params_1, model_params_2, closest_step = find_closest_checkpoint(fixed_step, load_dir)\n",
    "\n",
    "print(f\"Model restored successfully from step {closest_step}\")\n",
    "\n",
    "if 'mean_returns_per_seed' in locals() or 'mean_returns_per_seed' in globals():\n",
    "    return_value = f\"{mean_returns_per_seed[seed_idx][closest_step]:.3f}\".replace(\".\", \"_\")\n",
    "    print(f\"with mean return {float(mean_returns_per_seed[seed_idx][closest_step])}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of agents in environment: ['agent_0', 'agent_1']\n"
     ]
    }
   ],
   "source": [
    "# Set environment parameters\n",
    "max_steps = 1000\n",
    "key = jax.random.PRNGKey(0)\n",
    "key, key_r, key_a = jax.random.split(key, 3)\n",
    "\n",
    "# Choose layout\n",
    "#layout = overcooked_layouts[\"cramped_room\"]\n",
    "layout = custom_layout\n",
    "\n",
    "# Instantiate environment\n",
    "env = make('overcooked', layout=layout, max_steps=max_steps)\n",
    "\n",
    "# Reset environment\n",
    "obs, state = env.reset(key_r)\n",
    "print('List of agents in environment:', env.agents)\n",
    "\n",
    "# Visualization setup\n",
    "viz = OvercookedVisualizer()\n",
    "state_seq = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run environment loop using the trained model\n",
    "for _ in range(max_steps):\n",
    "    state_seq.append(state)\n",
    "    \n",
    "    # Get model-based actions\n",
    "    key, key_s = jax.random.split(key)\n",
    "    \n",
    "    actions = {}\n",
    "    for i, agent in enumerate(env.agents):\n",
    "        agent_obs = obs[agent]  # Extract observation for each agent\n",
    "        \n",
    "        # Choose the correct network for the current agent\n",
    "        if i == 0:\n",
    "            action_logits, value = network_1.apply(model_params_1, agent_obs.flatten())  # Get model's action distribution\n",
    "        elif i == 1:\n",
    "            action_logits, value = network_2.apply(model_params_2, agent_obs.flatten())  # Get model's action distribution\n",
    "        \n",
    "        action = action_logits.sample(seed=key_s)\n",
    "        key, key_s = jax.random.split(key)\n",
    "\n",
    "        actions[agent] = action\n",
    "    \n",
    "    # Step environment\n",
    "    obs, state, rewards, dones, infos = env.step(key_s, state, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animation saved to /mnt/lustre/home/samuloza/data/samuel_lozano/hfsp_collective_learning/data_overcooked_original/Asymmetric_Agents/custom_13/Checkpoints_2025-04-03_01-04-00/Seed_0//animation_custom_13_trained_model_2025-04-03_01-04-00_step_26200_value_128_500.gif with adjusted speed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuloza/.conda/envs/JaxMARL/lib/python3.12/subprocess.py:1893: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _fork_exec(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MP4 saved to /mnt/lustre/home/samuloza/data/samuel_lozano/hfsp_collective_learning/data_overcooked_original/Asymmetric_Agents/custom_13/Checkpoints_2025-04-03_01-04-00/Seed_0//animation_custom_13_trained_model_2025-04-03_01-04-00_step_26200_value_128_500.mp4\n"
     ]
    }
   ],
   "source": [
    "# FIX VISUALIZATION (SAVING IT WORKS)\n",
    "\n",
    "# Render to screen\n",
    "#for s in state_seq:\n",
    "#    viz.render(env.agent_view_size, s, highlight=False)\n",
    "#    time.sleep(0.1)\n",
    "\n",
    "# Save animation\n",
    "agent_view_size = 5\n",
    "\n",
    "output_filename = f\"{load_dir}/animation_{load_layout_name}_trained_model_{load_datetime}_step_{closest_step}_value_{return_value}.gif\"\n",
    "\n",
    "custom_animate(state_seq, agent_view_size=agent_view_size, filename=output_filename)\n",
    "\n",
    "print(f\"Animation saved to {output_filename} with adjusted speed\")\n",
    "\n",
    "gif_to_mp4(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JaxMARL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
