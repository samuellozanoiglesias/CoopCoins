{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import numpy as np\n",
    "from jaxmarl.environments.coin_game.make_train_RLLIB import make_train_RLLIB        \n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Hiperpar√°metros\n",
    "NUM_ENVS = 4\n",
    "NUM_INNER_STEPS = 200\n",
    "NUM_EPOCHS = 3000\n",
    "NUM_AGENTS = 2\n",
    "SHOW_EVERY_N_EPOCHS = 100\n",
    "SAVE_EVERY_N_EPOCHS = 500\n",
    "LR = 1e-4\n",
    "PAYOFF_MATRIX = [[1, 2, -3], [1, 2, -3]]\n",
    "GRID_SIZE = 3\n",
    "REWARD_COEF = [[1, 0], [1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/lustre/home/samuloza/.conda/envs/JaxMARL_TFM/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _posixsubprocess.fork_exec(\n",
      "2025-06-19 18:46:37,455\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m 2025-06-19 18:46:47,572\tINFO policy.py:1234 -- Policy (worker=1) running on CPU.\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m 2025-06-19 18:46:47,573\tINFO torch_policy_v2.py:105 -- Found 0 visible cuda devices.\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m 2025-06-19 18:46:49,673\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m 2025-06-19 18:46:49,673\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m         StateBufferConnector\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m 2025-06-19 18:46:49,673\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m         ImmutableActionsConnector\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m 2025-06-19 18:46:49,673\tINFO util.py:118 -- Using connectors:\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m 2025-06-19 18:46:49,673\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m         ObsPreprocessorConnector\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m         StateBufferConnector\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m         ViewRequirementAgentConnector\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m 2025-06-19 18:46:49,673\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m         ConvertToNumpyConnector\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m         NormalizeActionsConnector\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m         ImmutableActionsConnector\n",
      "2025-06-19 18:46:50,377\tINFO env_runner_group.py:320 -- Inferred observation/action spaces from remote worker (local worker has no env): {'agent_0': (Box(0, 1, (36,), uint8), Discrete(5)), 'agent_1': (Box(0, 1, (36,), uint8), Discrete(5)), '__env__': (<bound method CoinGameRLLibEnv.observation_space of <jaxmarl.environments.coin_game.coin_game_rllib_env.CoinGameRLLibEnv object at 0x7f279891cfd0>>, <bound method CoinGameRLLibEnv.action_space of <jaxmarl.environments.coin_game.coin_game_rllib_env.CoinGameRLLibEnv object at 0x7f279891cfd0>>)}\n",
      "2025-06-19 18:46:50,408\tINFO policy.py:1234 -- Policy (worker=local) running on CPU.\n",
      "2025-06-19 18:46:50,409\tINFO torch_policy_v2.py:105 -- Found 0 visible cuda devices.\n",
      "2025-06-19 18:46:52,046\tINFO policy.py:1234 -- Policy (worker=local) running on CPU.\n",
      "2025-06-19 18:46:52,048\tINFO torch_policy_v2.py:105 -- Found 0 visible cuda devices.\n",
      "2025-06-19 18:46:52,060\tINFO util.py:118 -- Using connectors:\n",
      "2025-06-19 18:46:52,060\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "        ObsPreprocessorConnector\n",
      "        StateBufferConnector\n",
      "        ViewRequirementAgentConnector\n",
      "2025-06-19 18:46:52,061\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "        ConvertToNumpyConnector\n",
      "        NormalizeActionsConnector\n",
      "        ImmutableActionsConnector\n",
      "2025-06-19 18:46:52,062\tINFO util.py:118 -- Using connectors:\n",
      "2025-06-19 18:46:52,063\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "        ObsPreprocessorConnector\n",
      "        StateBufferConnector\n",
      "        ViewRequirementAgentConnector\n",
      "2025-06-19 18:46:52,064\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "        ConvertToNumpyConnector\n",
      "        NormalizeActionsConnector\n",
      "        ImmutableActionsConnector\n",
      "2025-06-19 18:46:52,064\tINFO rollout_worker.py:1742 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['agent_0', 'agent_1']>\n",
      "2025-06-19 18:46:52,065\tINFO rollout_worker.py:1743 -- Built preprocessor map: {'agent_0': None, 'agent_1': None}\n",
      "2025-06-19 18:46:52,065\tINFO rollout_worker.py:542 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "2025-06-19 18:46:52,093\tINFO policy.py:1234 -- Policy (worker=local) running on CPU.\n",
      "2025-06-19 18:46:52,096\tINFO torch_policy_v2.py:105 -- Found 0 visible cuda devices.\n",
      "2025-06-19 18:46:52,112\tINFO policy.py:1234 -- Policy (worker=local) running on CPU.\n",
      "2025-06-19 18:46:52,113\tINFO torch_policy_v2.py:105 -- Found 0 visible cuda devices.\n",
      "2025-06-19 18:46:52,121\tINFO util.py:118 -- Using connectors:\n",
      "2025-06-19 18:46:52,122\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "        ObsPreprocessorConnector\n",
      "        StateBufferConnector\n",
      "        ViewRequirementAgentConnector\n",
      "2025-06-19 18:46:52,123\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "        ConvertToNumpyConnector\n",
      "        NormalizeActionsConnector\n",
      "        ImmutableActionsConnector\n",
      "2025-06-19 18:46:52,124\tINFO util.py:118 -- Using connectors:\n",
      "2025-06-19 18:46:52,125\tINFO util.py:119 --     AgentConnectorPipeline\n",
      "        ObsPreprocessorConnector\n",
      "        StateBufferConnector\n",
      "        ViewRequirementAgentConnector\n",
      "2025-06-19 18:46:52,125\tINFO util.py:120 --     ActionConnectorPipeline\n",
      "        ConvertToNumpyConnector\n",
      "        NormalizeActionsConnector\n",
      "        ImmutableActionsConnector\n",
      "2025-06-19 18:46:52,126\tINFO rollout_worker.py:1742 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['agent_0', 'agent_1']>\n",
      "2025-06-19 18:46:52,126\tINFO rollout_worker.py:1743 -- Built preprocessor map: {'agent_0': None, 'agent_1': None}\n",
      "2025-06-19 18:46:52,127\tINFO rollout_worker.py:542 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {})\n",
      "2025-06-19 18:46:52,142\tINFO env_runner_group.py:320 -- Inferred observation/action spaces from remote worker (local worker has no env): {'agent_0': (Box(0, 1, (36,), uint8), Discrete(5)), 'agent_1': (Box(0, 1, (36,), uint8), Discrete(5)), '__env__': (<bound method CoinGameRLLibEnv.observation_space of <jaxmarl.environments.coin_game.coin_game_rllib_env.CoinGameRLLibEnv object at 0x7f2430327d90>>, <bound method CoinGameRLLibEnv.action_space of <jaxmarl.environments.coin_game.coin_game_rllib_env.CoinGameRLLibEnv object at 0x7f2430327d90>>)}\n",
      "2025-06-19 18:46:52,143\tINFO trainable.py:160 -- Trainable.setup took 13.419 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m 2025-06-19 18:46:52,277\tINFO rollout_worker.py:671 -- Generating sample batch of size 200\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m 2025-06-19 18:46:57,108\tINFO rollout_worker.py:713 -- Completed sample batch:\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m { 'count': 200,\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m   'policy_batches': { 'agent_0': { 'action_dist_inputs': np.ndarray((200, 5), dtype=float32, min=-0.011, max=0.006, mean=-0.001),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'action_logp': np.ndarray((200,), dtype=float32, min=-1.618, max=-1.604, mean=-1.609),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'actions': np.ndarray((200,), dtype=int32, min=0.0, max=4.0, mean=2.075),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'advantages': np.ndarray((200,), dtype=float32, min=-11.844, max=4.66, mean=-1.24),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'eps_id': np.ndarray((200,), dtype=int64, min=7.860742320314758e+17, max=7.860742320314758e+17, mean=7.860742320314758e+17),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'new_obs': np.ndarray((200, 36), dtype=int8, min=0.0, max=1.0, mean=0.111),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'obs': np.ndarray((200, 36), dtype=int8, min=0.0, max=1.0, mean=0.111),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'rewards': np.ndarray((200,), dtype=float32, min=-3.0, max=2.0, mean=-0.07),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    't': np.ndarray((200,), dtype=int64, min=0.0, max=199.0, mean=99.5),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'terminateds': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.005),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'truncateds': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'unroll_id': np.ndarray((200,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'value_targets': np.ndarray((200,), dtype=float32, min=-11.847, max=4.658, mean=-1.241),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'values_bootstrapped': np.ndarray((200,), dtype=float32, min=-0.004, max=0.003, mean=-0.001),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'vf_preds': np.ndarray((200,), dtype=float32, min=-0.004, max=0.003, mean=-0.001)},\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                       'agent_1': { 'action_dist_inputs': np.ndarray((200, 5), dtype=float32, min=-0.004, max=0.008, mean=0.002),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'action_logp': np.ndarray((200,), dtype=float32, min=-1.615, max=-1.605, mean=-1.609),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'actions': np.ndarray((200,), dtype=int32, min=0.0, max=4.0, mean=2.05),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'advantages': np.ndarray((200,), dtype=float32, min=-8.006, max=10.192, mean=1.566),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'agent_index': np.ndarray((200,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'eps_id': np.ndarray((200,), dtype=int64, min=7.860742320314758e+17, max=7.860742320314758e+17, mean=7.860742320314758e+17),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'new_obs': np.ndarray((200, 36), dtype=int8, min=0.0, max=1.0, mean=0.111),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'obs': np.ndarray((200, 36), dtype=int8, min=0.0, max=1.0, mean=0.111),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'rewards': np.ndarray((200,), dtype=float32, min=-3.0, max=2.0, mean=0.1),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    't': np.ndarray((200,), dtype=int64, min=0.0, max=199.0, mean=99.5),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'terminateds': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.005),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'truncateds': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'unroll_id': np.ndarray((200,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'value_targets': np.ndarray((200,), dtype=float32, min=-8.004, max=10.19, mean=1.566),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'values_bootstrapped': np.ndarray((200,), dtype=float32, min=-0.005, max=0.002, mean=-0.001),\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m                                    'vf_preds': np.ndarray((200,), dtype=float32, min=-0.005, max=0.002, mean=-0.001)}},\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[36m(RolloutWorker pid=824816)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=824820)\u001b[0m 2025-06-19 18:46:50,356\tINFO policy.py:1234 -- Policy (worker=3) running on CPU.\u001b[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=824820)\u001b[0m 2025-06-19 18:46:50,356\tINFO torch_policy_v2.py:105 -- Found 0 visible cuda devices.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=824820)\u001b[0m 2025-06-19 18:46:50,366\tINFO util.py:118 -- Using connectors:\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=824820)\u001b[0m 2025-06-19 18:46:50,366\tINFO util.py:119 --     AgentConnectorPipeline\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=824820)\u001b[0m         ObsPreprocessorConnector\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=824820)\u001b[0m         StateBufferConnector\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=824820)\u001b[0m         ViewRequirementAgentConnector\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=824820)\u001b[0m 2025-06-19 18:46:50,366\tINFO util.py:120 --     ActionConnectorPipeline\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=824820)\u001b[0m         ConvertToNumpyConnector\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=824820)\u001b[0m         NormalizeActionsConnector\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=824820)\u001b[0m         ImmutableActionsConnector\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2025-06-19 18:46:57,293\tWARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "2025-06-19 18:46:57,296\tINFO rollout_worker.py:790 -- Training on concatenated sample batches:\n",
      "\n",
      "{ 'count': 32,\n",
      "  'policy_batches': { 'agent_0': { 'action_dist_inputs': np.ndarray((32, 5), dtype=float32, min=-0.009, max=0.009, mean=-0.0),\n",
      "                                   'action_logp': np.ndarray((32,), dtype=float32, min=-1.618, max=-1.602, mean=-1.609),\n",
      "                                   'actions': np.ndarray((32,), dtype=int32, min=0.0, max=4.0, mean=2.5),\n",
      "                                   'advantages': np.ndarray((32,), dtype=float32, min=-2.02, max=1.836, mean=0.226),\n",
      "                                   'agent_index': np.ndarray((32,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "                                   'eps_id': np.ndarray((32,), dtype=int64, min=1.3862635850362939e+17, max=8.13821169387857e+17, mean=5.5207118756034426e+17),\n",
      "                                   'infos': np.ndarray((32,), dtype=object, head={'agent_id': 'agent_0', 'cumulated_pure_reward': -26.0, 'cumulated_modified_reward': -26.0, 'cumulated_action_stats': np.ndarray((5,), dtype=int32, min=6.0, max=75.0, mean=33.6)}),\n",
      "                                   'new_obs': np.ndarray((32, 36), dtype=int8, min=0.0, max=1.0, mean=0.111),\n",
      "                                   'obs': np.ndarray((32, 36), dtype=int8, min=0.0, max=1.0, mean=0.111),\n",
      "                                   'rewards': np.ndarray((32,), dtype=float32, min=-3.0, max=2.0, mean=-0.125),\n",
      "                                   't': np.ndarray((32,), dtype=int64, min=1.0, max=198.0, mean=92.406),\n",
      "                                   'terminateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "                                   'truncateds': np.ndarray((32,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "                                   'unroll_id': np.ndarray((32,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "                                   'value_targets': np.ndarray((32,), dtype=float32, min=-10.295, max=5.366, mean=-1.173),\n",
      "                                   'values_bootstrapped': np.ndarray((32,), dtype=float32, min=-0.004, max=0.007, mean=0.001),\n",
      "                                   'vf_preds': np.ndarray((32,), dtype=float32, min=-0.004, max=0.007, mean=0.001)}},\n",
      "  'type': 'MultiAgentBatch'}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete.\n",
      "Checkpoint saved at TrainingResult(checkpoint=Checkpoint(filesystem=local, path=/mnt/lustre/home/samuloza/data/samuel_lozano/coin_game/pruebas/Prisioner_dilemma/Training_2025-06-19_18-46-35/checkpoint_0), metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'agent_0': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 0.21162985563278197, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 9.999999999999999e-05, 'total_loss': 2.9253132786069598, 'policy_loss': -0.003194301894732884, 'vf_loss': 5.889199311392648, 'vf_explained_var': 8.653742926461356e-05, 'kl': 1.1036734070200639e-05, 'entropy': 1.6094321863991874, 'entropy_coeff': 0.01}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 114.28571428571429, 'num_grad_updates_lifetime': 18.0, 'diff_num_grad_updates_vs_sampler_policy': 17.0}, 'agent_1': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 0.26901841461658477, 'cur_kl_coeff': 0.20000000000000004, 'cur_lr': 9.999999999999999e-05, 'total_loss': 2.8337703704833985, 'policy_loss': -0.0032033394489969524, 'vf_loss': 5.706134305681501, 'vf_explained_var': -2.002034868512835e-05, 'kl': 4.402382994937528e-06, 'entropy': 1.6094357013702392, 'entropy_coeff': 0.01}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 114.28571428571429, 'num_grad_updates_lifetime': 18.0, 'diff_num_grad_updates_vs_sampler_policy': 17.0}}, 'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600}, 'env_runners': {'episode_reward_max': 6.0, 'episode_reward_min': -26.0, 'episode_reward_mean': -7.5, 'episode_len_mean': 200.0, 'episode_media': {}, 'episodes_timesteps_total': 800, 'policy_reward_min': {'agent_0': -44.0, 'agent_1': -3.0}, 'policy_reward_max': {'agent_0': -3.0, 'agent_1': 38.0}, 'policy_reward_mean': {'agent_0': -25.75, 'agent_1': 18.25}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [6.0, -26.0, -6.0, -4.0], 'episode_lengths': [200, 200, 200, 200], 'policy_agent_0_reward': [-14.0, -44.0, -3.0, -42.0], 'policy_agent_1_reward': [20.0, 18.0, -3.0, 38.0]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.8592667864329778, 'mean_inference_ms': 2.8363507778490358, 'mean_action_processing_ms': 0.25914527883577104, 'mean_env_wait_ms': 20.268181959788002, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.016570091247558594, 'StateBufferConnector_ms': 0.00591278076171875, 'ViewRequirementAgentConnector_ms': 0.1467496156692505}, 'num_episodes': 4, 'episode_return_max': 6.0, 'episode_return_min': -26.0, 'episode_return_mean': -7.5, 'episodes_this_iter': 4}, 'num_healthy_workers': 4, 'actor_manager_num_outstanding_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600, 'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_env_steps_sampled_this_iter': 800, 'num_env_steps_trained_this_iter': 800, 'num_env_steps_sampled_throughput_per_sec': 138.34191633946907, 'num_env_steps_trained_throughput_per_sec': 138.34191633946907, 'timesteps_total': 800, 'num_env_steps_sampled_lifetime': 800, 'num_agent_steps_sampled_lifetime': 1600, 'num_steps_trained_this_iter': 800, 'agent_timesteps_total': 1600, 'timers': {'training_iteration_time_ms': 5782.789, 'restore_workers_time_ms': 0.022, 'training_step_time_ms': 5782.699, 'sample_time_ms': 5016.942, 'learn_time_ms': 747.356, 'learn_throughput': 1070.44, 'synch_weights_time_ms': 16.977}, 'counters': {'num_env_steps_sampled': 800, 'num_env_steps_trained': 800, 'num_agent_steps_sampled': 1600, 'num_agent_steps_trained': 1600}, 'done': False, 'training_iteration': 1, 'trial_id': 'default', 'date': '2025-06-19_18-46-58', 'timestamp': 1750351618, 'time_this_iter_s': 5.787264108657837, 'time_total_s': 5.787264108657837, 'pid': 824296, 'hostname': 'login1', 'node_ip': '147.96.240.246', 'config': {'exploration_config': {'type': 'StochasticSampling'}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'coin_game_env_RLLIB', 'env_config': {'NUM_ENVS': 4, 'NUM_INNER_STEPS': 200, 'NUM_EPOCHS': 3000, 'NUM_AGENTS': 2, 'SHOW_EVERY_N_EPOCHS': 100, 'SAVE_EVERY_N_EPOCHS': 500, 'LR': 0.0001, 'PAYOFF_MATRIX': [[1, 2, -3], [1, 2, -3]], 'GRID_SIZE': 3, 'REWARD_COEF': [[1, 0], [1, 0]], 'SAVE_DIR': '/mnt/lustre/home/samuloza/data/samuel_lozano/coin_game/pruebas/Prisioner_dilemma/', 'NUM_UPDATES': 5, 'GAMMA': 0.995, 'GAE_LAMBDA': 0.95, 'ENT_COEF': 0.01, 'CLIP_EPS': 0.2, 'VF_COEF': 0.5, 'PATH': '/mnt/lustre/home/samuloza/data/samuel_lozano/coin_game/pruebas/Prisioner_dilemma/Training_2025-06-19_18-46-35'}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 4, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 0, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.995, 'lr': 0.0001, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 800, 'num_epochs': 5, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [64, 64, 16], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initializer': None, 'lstm_weights_initializer_config': None, 'lstm_bias_initializer': None, 'lstm_bias_initializer_config': None, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1, '_disable_preprocessor_api': False, '_disable_action_flattening': False}, '_learner_connector': None, 'add_default_connectors_to_learner_pipeline': True, 'learner_config_dict': {}, 'optimizer': {}, '_learner_class': None, 'callbacks_on_algorithm_init': None, 'callbacks_on_env_runners_recreated': None, 'callbacks_on_offline_eval_runners_recreated': None, 'callbacks_on_checkpoint_loaded': None, 'callbacks_on_environment_created': None, 'callbacks_on_episode_created': None, 'callbacks_on_episode_start': None, 'callbacks_on_episode_step': None, 'callbacks_on_episode_end': None, 'callbacks_on_evaluate_start': None, 'callbacks_on_evaluate_end': None, 'callbacks_on_evaluate_offline_start': None, 'callbacks_on_evaluate_offline_end': None, 'callbacks_on_sample_end': None, 'callbacks_on_train_result': None, 'explore': True, 'enable_rl_module_and_learner': False, 'enable_env_runner_and_connector_v2': False, '_prior_exploration_config': None, 'count_steps_by': 'env_steps', 'policy_map_capacity': 100, 'policy_mapping_fn': <function make_train_RLLIB.<locals>.<lambda> at 0x7f2798bf16c0>, 'policies_to_train': ['agent_0', 'agent_1'], 'policy_states_are_swappable': False, 'observation_fn': None, 'offline_data_class': None, 'input_read_method': 'read_parquet', 'input_read_method_kwargs': {}, 'input_read_schema': {}, 'input_read_episodes': False, 'input_read_sample_batches': False, 'input_read_batch_size': None, 'input_filesystem': None, 'input_filesystem_kwargs': {}, 'input_compress_columns': ['obs', 'new_obs'], 'input_spaces_jsonable': True, 'materialize_data': False, 'materialize_mapped_data': True, 'map_batches_kwargs': {}, 'iter_batches_kwargs': {}, 'ignore_final_observation': False, 'prelearner_class': None, 'prelearner_buffer_class': None, 'prelearner_buffer_kwargs': {}, 'prelearner_module_synch_period': 10, 'dataset_num_iters_per_learner': None, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'output_max_rows_per_file': None, 'output_write_remaining_data': False, 'output_write_method': 'write_parquet', 'output_write_method_kwargs': {}, 'output_filesystem': None, 'output_filesystem_kwargs': {}, 'output_write_episodes': True, 'offline_sampling': False, 'evaluation_interval': 100, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 120.0, 'evaluation_auto_duration_min_env_steps_per_sample': 100, 'evaluation_auto_duration_max_env_steps_per_sample': 2000, 'evaluation_parallel_to_training': False, 'evaluation_force_reset_envs_before_iteration': True, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_env_runners': 0, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 10.0, 'offline_evaluation_interval': None, 'num_offline_eval_runners': 0, 'offline_loss_for_module_fn': None, 'offline_evaluation_duration': 1, 'offline_evaluation_parallel_to_training': False, 'offline_evaluation_timeout_s': 120.0, 'num_cpus_per_offline_eval_runner': 1, 'num_gpus_per_offline_eval_runner': 0, 'custom_resources_per_offline_eval_runner': {}, 'restart_failed_offline_eval_runners': True, 'ignore_offline_eval_runner_failures': False, 'max_num_offline_eval_runner_restarts': 1000, 'offline_eval_runner_restore_timeout_s': 1800.0, 'max_requests_in_flight_per_offline_eval_runner': 1, 'validate_offline_eval_runners_after_construction': True, 'offline_eval_runner_health_probe_timeout_s': 30.0, 'offline_eval_rl_module_inference_only': False, 'broadcast_offline_eval_runner_states': False, 'offline_eval_batch_size_per_runner': 256, 'dataset_num_iters_per_eval_runner': 1, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'log_gradients': True, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'INFO', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'restart_failed_env_runners': True, 'ignore_env_runner_failures': False, 'max_num_env_runner_restarts': 1000, 'delay_between_env_runner_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_env_runner_failures_tolerance': 100, 'env_runner_health_probe_timeout_s': 30.0, 'env_runner_restore_timeout_s': 1800.0, '_model_config': {}, '_rl_module_spec': None, 'algorithm_config_overrides_per_module': {}, '_per_module_overrides': {}, '_validate_config': True, '_use_msgpack_checkpoints': False, '_torch_grad_scaler_class': None, '_torch_lr_scheduler_classes': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_initialize_loss_from_dummy_batch': False, '_dont_auto_sync_env_runner_states': False, 'env_task_fn': -1, 'enable_connectors': -1, 'simple_optimizer': True, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'enable_async_evaluation': -1, 'custom_async_evaluation_function': -1, '_enable_rl_module_api': -1, 'auto_wrap_old_gym_envs': -1, 'always_attach_evaluation_results': -1, 'replay_sequence_length': None, '_disable_execution_plan_api': -1, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'vf_loss_coeff': 0.5, 'entropy_coeff': 0.01, 'clip_param': 0.2, 'vf_clip_param': 10.0, 'entropy_coeff_schedule': None, 'lr_schedule': None, 'sgd_minibatch_size': -1, 'vf_share_layers': -1, 'class': <class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>, 'lambda': 0.95, 'input': 'sampler', 'policies': {'agent_0': (None, Box(0, 1, (36,), uint8), Discrete(5), {}), 'agent_1': (None, Box(0, 1, (36,), uint8), Discrete(5), {})}, 'callbacks': <class 'ray.rllib.callbacks.callbacks.RLlibCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch'}, 'time_since_restore': 5.787264108657837, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': 17.22222222222222, 'ram_util_percent': 20.577777777777776}})\n"
     ]
    }
   ],
   "source": [
    "brigit = '/mnt/lustre/home/samuloza'\n",
    "save_dir = f'{brigit}/data/samuel_lozano/coin_game/pruebas/Prisioner_dilemma/'\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "config = {\n",
    "    \"NUM_ENVS\": NUM_ENVS,\n",
    "    \"NUM_INNER_STEPS\": NUM_INNER_STEPS,\n",
    "    \"NUM_EPOCHS\": NUM_EPOCHS,\n",
    "    \"NUM_AGENTS\": NUM_AGENTS,\n",
    "    \"SHOW_EVERY_N_EPOCHS\": SHOW_EVERY_N_EPOCHS,\n",
    "    \"SAVE_EVERY_N_EPOCHS\": SAVE_EVERY_N_EPOCHS,\n",
    "    \"LR\": LR,\n",
    "    \"PAYOFF_MATRIX\": PAYOFF_MATRIX,\n",
    "    \"GRID_SIZE\": GRID_SIZE,\n",
    "    \"REWARD_COEF\": REWARD_COEF,\n",
    "    \"SAVE_DIR\": save_dir,\n",
    "    # RLlib specific parameters\n",
    "    \"NUM_UPDATES\": 5,  # Number of updates of the policy\n",
    "    \"GAMMA\": 0.995,  # Discount factor\n",
    "    \"GAE_LAMBDA\": 0.95,  # GAE-Lambda parameter\n",
    "    \"ENT_COEF\": 0.01,  # Entropy coefficient\n",
    "    \"CLIP_EPS\": 0.2,  # PPO clip parameter\n",
    "    \"VF_COEF\": 0.5  # Value function coefficient\n",
    "}\n",
    "\n",
    "trainer, current_date = make_train_RLLIB(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JaxMARL_TFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
