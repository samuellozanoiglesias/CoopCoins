{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoinGame Visualizer\n",
    "import os\n",
    "import sys\n",
    "import ray\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from jaxmarl.environments.coin_game.coin_game_rllib_env import CoinGameRLLibEnv\n",
    "from jaxmarl.environments.coin_game.coin_game import CoinGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rllib_checkpoint(checkpoint_path):\n",
    "    \"\"\"Load a trained RLlib checkpoint.\"\"\"\n",
    "    from ray.rllib.algorithms.ppo import PPO\n",
    "    \n",
    "    # Initialize Ray if not already done\n",
    "    if not ray.is_initialized():\n",
    "        ray.init(ignore_reinit_error=True, include_dashboard=False)\n",
    "    \n",
    "    # Load the checkpoint\n",
    "    trainer = PPO.from_checkpoint(checkpoint_path)\n",
    "    return trainer\n",
    "\n",
    "def create_visualization_env(config):\n",
    "    \"\"\"Create environment with same config as training.\"\"\"\n",
    "    return CoinGameRLLibEnv(\n",
    "        num_inner_steps=config[\"NUM_INNER_STEPS\"],\n",
    "        num_outer_steps=config[\"NUM_EPOCHS\"],\n",
    "        cnn=False,\n",
    "        egocentric=False,\n",
    "        payoff_matrix=config[\"PAYOFF_MATRIX\"],\n",
    "        grid_size=config[\"GRID_SIZE\"],\n",
    "        reward_coef=config[\"REWARD_COEF\"],\n",
    "        path=\"temp_vis\",\n",
    "        env_idx=0\n",
    "    )\n",
    "\n",
    "def render_state_using_original_method(state, grid_size, step_info=None):\n",
    "    \"\"\"Render the current state using the original coin_game.py render method.\"\"\"\n",
    "    import numpy as np\n",
    "    from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "    from matplotlib.figure import Figure\n",
    "    from PIL import Image\n",
    "\n",
    "    \"\"\"Small utility for plotting the agent's state.\"\"\"\n",
    "    fig = Figure((8, 4))\n",
    "    canvas = FigureCanvas(fig)\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.imshow(\n",
    "        np.zeros((grid_size, grid_size)),\n",
    "        cmap=\"Greys\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        aspect=\"equal\",\n",
    "        interpolation=\"none\",\n",
    "        origin=\"lower\",\n",
    "        extent=[0, grid_size, 0, grid_size],\n",
    "    )\n",
    "    ax.set_aspect(\"equal\")\n",
    "\n",
    "    # ax.margins(0)\n",
    "    ax.set_xticks(jnp.arange(1, grid_size + 1))\n",
    "    ax.set_yticks(jnp.arange(1, grid_size + 1))\n",
    "    ax.grid()\n",
    "    red_pos = jnp.squeeze(state.red_pos)\n",
    "    blue_pos = jnp.squeeze(state.blue_pos)\n",
    "    red_coin_pos = jnp.squeeze(state.red_coin_pos)\n",
    "    blue_coin_pos = jnp.squeeze(state.blue_coin_pos)\n",
    "    ax.annotate(\n",
    "        \"R\",\n",
    "        fontsize=20,\n",
    "        color=\"red\",\n",
    "        xy=(red_pos[0], red_pos[1]),\n",
    "        xycoords=\"data\",\n",
    "        xytext=(red_pos[0] + 0.5, red_pos[1] + 0.5),\n",
    "    )\n",
    "    ax.annotate(\n",
    "        \"B\",\n",
    "        fontsize=20,\n",
    "        color=\"blue\",\n",
    "        xy=(blue_pos[0], blue_pos[1]),\n",
    "        xycoords=\"data\",\n",
    "        xytext=(blue_pos[0] + 0.5, blue_pos[1] + 0.5),\n",
    "    )\n",
    "    ax.annotate(\n",
    "        \"Rc\",\n",
    "        fontsize=20,\n",
    "        color=\"red\",\n",
    "        xy=(red_coin_pos[0], red_coin_pos[1]),\n",
    "        xycoords=\"data\",\n",
    "        xytext=(red_coin_pos[0] + 0.3, red_coin_pos[1] + 0.3),\n",
    "    )\n",
    "    ax.annotate(\n",
    "        \"Bc\",\n",
    "        color=\"blue\",\n",
    "        fontsize=20,\n",
    "        xy=(blue_coin_pos[0], blue_coin_pos[1]),\n",
    "        xycoords=\"data\",\n",
    "        xytext=(\n",
    "            blue_coin_pos[0] + 0.3,\n",
    "            blue_coin_pos[1] + 0.3,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.text(0.0, 0.95, \"Timestep: %s\" % (state.inner_t))\n",
    "    ax2.text(0.0, 0.75, \"Episode: %s\" % (state.outer_t))\n",
    "    \n",
    "    # Use the original statistics from the state\n",
    "    if hasattr(state, 'red_coop') and hasattr(state, 'red_defect'):\n",
    "        ax2.text(\n",
    "            0.0, 0.45, \"Red Coop: %s\" % (state.red_coop[state.outer_t].sum())\n",
    "        )\n",
    "        ax2.text(\n",
    "            0.6,\n",
    "            0.45,\n",
    "            \"Red Defects : %s\" % (state.red_defect[state.outer_t].sum()),\n",
    "        )\n",
    "        ax2.text(\n",
    "            0.0, 0.25, \"Blue Coop: %s\" % (state.blue_coop[state.outer_t].sum())\n",
    "        )\n",
    "        ax2.text(\n",
    "            0.6,\n",
    "            0.25,\n",
    "            \"Blue Defects : %s\" % (state.blue_defect[state.outer_t].sum()),\n",
    "        )\n",
    "        ax2.text(\n",
    "            0.0,\n",
    "            0.05,\n",
    "            \"Red Total: %s\"\n",
    "            % (\n",
    "                state.red_defect[state.outer_t].sum()\n",
    "                + state.red_coop[state.outer_t].sum()\n",
    "            ),\n",
    "        )\n",
    "        ax2.text(\n",
    "            0.6,\n",
    "            0.05,\n",
    "            \"Blue Total: %s\"\n",
    "            % (\n",
    "                state.blue_defect[state.outer_t].sum()\n",
    "                + state.blue_coop[state.outer_t].sum()\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        # Fallback to action stats if coop/defect stats not available\n",
    "        ax2.text(0.0, 0.45, \"Agent 0 (Red) Action Stats:\")\n",
    "        ax2.text(0.0, 0.35, \"  Own coins: %s\" % (state.action_stats[0][0]))\n",
    "        ax2.text(0.0, 0.25, \"  Other coins: %s\" % (state.action_stats[0][1]))\n",
    "        ax2.text(0.0, 0.15, \"  Reject own: %s\" % (state.action_stats[0][2]))\n",
    "        ax2.text(0.0, 0.05, \"  Reject other: %s\" % (state.action_stats[0][3]))\n",
    "        \n",
    "        ax2.text(0.6, 0.45, \"Agent 1 (Blue) Action Stats:\")\n",
    "        ax2.text(0.6, 0.35, \"  Own coins: %s\" % (state.action_stats[1][0]))\n",
    "        ax2.text(0.6, 0.25, \"  Other coins: %s\" % (state.action_stats[1][1]))\n",
    "        ax2.text(0.6, 0.15, \"  Reject own: %s\" % (state.action_stats[1][2]))\n",
    "        ax2.text(0.6, 0.05, \"  Reject other: %s\" % (state.action_stats[1][3]))\n",
    "    \n",
    "    # Add step info if provided\n",
    "    if step_info:\n",
    "        ax2.text(0.0, -0.1, f\"Red reward: {step_info.get('red_reward', 0):.2f}\", \n",
    "                transform=ax2.transAxes, fontsize=10)\n",
    "        ax2.text(0.0, -0.2, f\"Blue reward: {step_info.get('blue_reward', 0):.2f}\", \n",
    "                transform=ax2.transAxes, fontsize=10)\n",
    "        ax2.text(0.0, -0.3, f\"Red action: {step_info.get('red_action', 'N/A')}\", \n",
    "                transform=ax2.transAxes, fontsize=10)\n",
    "        ax2.text(0.0, -0.4, f\"Blue action: {step_info.get('blue_action', 'N/A')}\", \n",
    "                transform=ax2.transAxes, fontsize=10)\n",
    "    \n",
    "    ax2.axis(\"off\")\n",
    "    canvas.draw()\n",
    "    image = Image.frombytes(\n",
    "        \"RGB\",\n",
    "        fig.canvas.get_width_height(),\n",
    "        fig.canvas.tostring_rgb(),\n",
    "    )\n",
    "    return image\n",
    "\n",
    "def get_action_name(action):\n",
    "    \"\"\"Convert action index to name.\"\"\"\n",
    "    actions = [\"Right\", \"Left\", \"Up\", \"Down\", \"Stay\"]\n",
    "    return actions[action] if 0 <= action < len(actions) else f\"Unknown({action})\"\n",
    "\n",
    "def visualize_episode(trainer, config, num_episodes=1, save_gif=True, output_dir=\"visualizations\"):\n",
    "    \"\"\"Visualize one or more episodes using the trained model.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Create environment\n",
    "    env = create_visualization_env(config)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        print(f\"Visualizing episode {episode + 1}/{num_episodes}\")\n",
    "        \n",
    "        # Reset environment\n",
    "        obs, info = env.reset()\n",
    "        frames = []\n",
    "        step_infos = []\n",
    "        \n",
    "        # Get policies\n",
    "        policy_0 = trainer.get_policy(\"agent_0\")\n",
    "        policy_1 = trainer.get_policy(\"agent_1\")\n",
    "        \n",
    "        episode_rewards = {\"agent_0\": 0, \"agent_1\": 0}\n",
    "        \n",
    "        # Run episode\n",
    "        for step in range(config[\"NUM_INNER_STEPS\"]):\n",
    "            # Get actions from trained policies\n",
    "            action_0 = policy_0.compute_single_action(obs[\"agent_0\"])[0]\n",
    "            action_1 = policy_1.compute_single_action(obs[\"agent_1\"])[0]\n",
    "            \n",
    "            actions = {\"agent_0\": action_0, \"agent_1\": action_1}\n",
    "            \n",
    "            # Step environment\n",
    "            obs, rewards, terminated, truncated, info = env.step(actions)\n",
    "            \n",
    "            # Accumulate rewards\n",
    "            episode_rewards[\"agent_0\"] += rewards[\"agent_0\"]\n",
    "            episode_rewards[\"agent_1\"] += rewards[\"agent_1\"]\n",
    "            \n",
    "            # Create step info for visualization\n",
    "            step_info = {\n",
    "                'red_reward': rewards[\"agent_0\"],\n",
    "                'blue_reward': rewards[\"agent_1\"],\n",
    "                'red_action': get_action_name(action_0),\n",
    "                'blue_action': get_action_name(action_1),\n",
    "                'cumulative_red_reward': episode_rewards[\"agent_0\"],\n",
    "                'cumulative_blue_reward': episode_rewards[\"agent_1\"]\n",
    "            }\n",
    "            step_infos.append(step_info)\n",
    "            \n",
    "            # Render frame using the original render method\n",
    "            frame = render_state_using_original_method(env.state, config[\"GRID_SIZE\"], step_info)\n",
    "            frames.append(frame)\n",
    "            \n",
    "            # Check if episode is done\n",
    "            if terminated[\"__all__\"] or truncated[\"__all__\"]:\n",
    "                break\n",
    "        \n",
    "        # Save GIF\n",
    "        if save_gif:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            gif_path = os.path.join(output_dir, f\"episode_{episode}_{timestamp}.gif\")\n",
    "            \n",
    "            frames[0].save(\n",
    "                gif_path,\n",
    "                format=\"GIF\",\n",
    "                save_all=True,\n",
    "                append_images=frames[1:],\n",
    "                duration=500,  # 500ms per frame\n",
    "                loop=0,\n",
    "            )\n",
    "            print(f\"GIF saved: {gif_path}\")\n",
    "        \n",
    "        # Print episode summary\n",
    "        print(f\"Episode {episode + 1} Summary:\")\n",
    "        print(f\"  Total steps: {len(frames)}\")\n",
    "        print(f\"  Agent 0 (Red) total reward: {episode_rewards['agent_0']:.2f}\")\n",
    "        print(f\"  Agent 1 (Blue) total reward: {episode_rewards['agent_1']:.2f}\")\n",
    "        print(f\"  Agent 0 action stats: {env.state.action_stats[0]}\")\n",
    "        print(f\"  Agent 1 action stats: {env.state.action_stats[1]}\")\n",
    "        print()\n",
    "\n",
    "def load_config_from_file(config_path):\n",
    "    \"\"\"Load configuration from a saved config file.\"\"\"\n",
    "    config = {}\n",
    "    with open(config_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if ':' in line:\n",
    "                key, value = line.strip().split(':', 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                \n",
    "                # Try to convert to appropriate type\n",
    "                try:\n",
    "                    if value.startswith('[') and value.endswith(']'):\n",
    "                        # Handle lists\n",
    "                        value = eval(value)\n",
    "                    elif value.lower() in ['true', 'false']:\n",
    "                        value = value.lower() == 'true'\n",
    "                    elif '.' in value:\n",
    "                        value = float(value)\n",
    "                    else:\n",
    "                        value = int(value)\n",
    "                except:\n",
    "                    pass  # Keep as string if conversion fails\n",
    "                \n",
    "                config[key] = value\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sustituye esto con la ruta al checkpoint\n",
    "checkpoint_path = \"C:\\OneDrive - Universidad Complutense de Madrid (UCM)\\Doctorado\\data\\samuel_lozano\\coin_game\\RLLIB\\Prisioner_dilemma\\Training_2025-06-20_12-03-35\\checkpoint_4500\"\n",
    "\n",
    "# Configuraci칩n opcional\n",
    "config_path = None  # o por ejemplo: \"ruta/a/config.txt\"\n",
    "episodes = 3\n",
    "output_dir = \"C:\\OneDrive - Universidad Complutense de Madrid (UCM)\\Doctorado\\data\\samuel_lozano\\coin_game\\RLLIB\\Prisioner_dilemma\\Training_2025-06-20_12-03-35\\visualizations\"\n",
    "no_gif = False  # True para no guardar GIFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found config file: C:\\OneDrive - Universidad Complutense de Madrid (UCM)\\Doctorado\\data\\samuel_lozano\\coin_game\\RLLIB\\Prisioner_dilemma\\Training_2025-06-20_12-03-35\\config.txt\n"
     ]
    }
   ],
   "source": [
    "# Buscar archivo de configuraci칩n si no se proporciona\n",
    "if not config_path:\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    potential_config = os.path.join(checkpoint_dir, \"config.txt\")\n",
    "    if os.path.exists(potential_config):\n",
    "        config_path = potential_config\n",
    "        print(f\"Found config file: {config_path}\")\n",
    "    else:\n",
    "        print(\"Warning: No config file found. Using default values.\")\n",
    "        config_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration:\n",
      "  NUM_ENVS: 1\n",
      "  NUM_INNER_STEPS: 150\n",
      "  NUM_EPOCHS: 5000\n",
      "  NUM_AGENTS: 2\n",
      "  SHOW_EVERY_N_EPOCHS: 1000\n",
      "  SAVE_EVERY_N_EPOCHS: 500\n",
      "  LR: 0.0003\n",
      "  PAYOFF_MATRIX: [[1, 2, -3], [1, 2, -3]]\n",
      "  GRID_SIZE: 3\n",
      "  REWARD_COEF: [[0.707107, -0.707107], [1.0, 0.0]]\n",
      "  SAVE_DIR: /home/samuel_lozano/data/samuel_lozano/coin_game/RLLIB/Prisioner_dilemma\n",
      "  NUM_UPDATES: 4\n",
      "  GAMMA: 0.9\n",
      "  GAE_LAMBDA: 0.95\n",
      "  ENT_COEF: 0.05\n",
      "  CLIP_EPS: 0.2\n",
      "  VF_COEF: 0.5\n",
      "  PATH: /home/samuel_lozano/data/samuel_lozano/coin_game/RLLIB/Prisioner_dilemma/Training_2025-06-20_12-03-35\n"
     ]
    }
   ],
   "source": [
    "if config_path and os.path.exists(config_path):\n",
    "    config = load_config_from_file(config_path)\n",
    "    print(\"Loaded configuration:\")\n",
    "    for key, value in config.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    config = {\n",
    "        \"NUM_INNER_STEPS\": 10,\n",
    "        \"NUM_EPOCHS\": 10,\n",
    "        \"PAYOFF_MATRIX\": [[1, 1, -2], [1, 1, -2]],\n",
    "        \"GRID_SIZE\": 3,\n",
    "        \"REWARD_COEF\": [[1, 0], [1, 0]]\n",
    "    }\n",
    "    print(\"Using default configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: C:\\OneDrive - Universidad Complutense de Madrid (UCM)\\Doctorado\\data\\samuel_lozano\\coin_game\\RLLIB\\Prisioner_dilemma\\Training_2025-06-20_12-03-35\\checkpoint_4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 21:47:36,555\tERROR services.py:1355 -- Failed to start the dashboard , return code 3221226505\n",
      "2025-06-21 21:47:36,555\tERROR services.py:1380 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure' to find where the log file is.\n",
      "2025-06-21 21:47:36,586\tERROR services.py:1424 -- \n",
      "The last 20 lines of C:\\Users\\samul\\AppData\\Local\\Temp\\ray\\session_2025-06-21_21-47-33_573864_22380\\logs\\dashboard.log (it contains the error message from the dashboard): \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\samul\\anaconda3\\envs\\JaxMARL_TFM\\lib\\site-packages\\ray\\dashboard\\dashboard.py\", line 246, in <module>\n",
      "    logging_utils.redirect_stdout_stderr_if_needed(\n",
      "  File \"c:\\Users\\samul\\anaconda3\\envs\\JaxMARL_TFM\\lib\\site-packages\\ray\\_private\\logging_utils.py\", line 49, in redirect_stdout_stderr_if_needed\n",
      "    sys.stderr = open_log(stderr_fileno, unbuffered=True, closefd=False)\n",
      "  File \"c:\\Users\\samul\\anaconda3\\envs\\JaxMARL_TFM\\lib\\site-packages\\ray\\_private\\utils.py\", line 444, in open_log\n",
      "    stream = open(path, **kwargs)\n",
      "OSError: [WinError 6] Controlador no v치lido\n",
      "\n",
      "\n",
      "2025-06-21 21:47:36,913\tINFO worker.py:1917 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "# Cargar modelo\n",
    "print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "trainer = load_rllib_checkpoint(checkpoint_path)\n",
    "\n",
    "# Visualizar episodios\n",
    "visualize_episode(\n",
    "    trainer,\n",
    "    config,\n",
    "    num_episodes=episodes,\n",
    "    save_gif=not no_gif,\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "# Apagar Ray si est치 inicializado\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JaxMARL_TFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
