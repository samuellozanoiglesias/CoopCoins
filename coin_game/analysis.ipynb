{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bfe3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import numpy as np\n",
    "from jaxmarl.environments.coin_game.make_train import make_train       \n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63757f7d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "012e18fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros\n",
    "NUM_ENVS = 6\n",
    "NUM_INNER_STEPS = 100\n",
    "NUM_UPDATES_PER_EPOCH = 8\n",
    "NUM_EPOCHS = 3000\n",
    "NUM_AGENTS = 2\n",
    "SHOW_EVERY_N_EPOCHS = 100\n",
    "SAVE_EVERY_N_EPOCHS = 500\n",
    "LR = 1e-5\n",
    "PAYOFF_MATRIX = [[1, 2, -3], [1, 2, -3]]\n",
    "GRID_SIZE = 3\n",
    "REWARD_COEF = [[1, 0], [1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f430ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#local = '/mnt/lustre/home/samuloza'\n",
    "local = 'D:/OneDrive - Universidad Complutense de Madrid (UCM)/Doctorado'\n",
    "save_dir = f'{local}/data/samuel_lozano/coin_game/pruebas/Prisioner_dilemma/'\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "config = {\n",
    "    \"NUM_ENVS\": NUM_ENVS,\n",
    "    \"NUM_INNER_STEPS\": NUM_INNER_STEPS,\n",
    "    \"NUM_EPOCHS\": NUM_EPOCHS,\n",
    "    \"NUM_AGENTS\": NUM_AGENTS,\n",
    "    \"SHOW_EVERY_N_EPOCHS\": SHOW_EVERY_N_EPOCHS,\n",
    "    \"SAVE_EVERY_N_EPOCHS\": SAVE_EVERY_N_EPOCHS,\n",
    "    \"LR\": LR,\n",
    "    \"PAYOFF_MATRIX\": PAYOFF_MATRIX,\n",
    "    \"GRID_SIZE\": GRID_SIZE,\n",
    "    \"REWARD_COEF\": REWARD_COEF,\n",
    "    \"SAVE_DIR\": save_dir,\n",
    "    \"GAMMA\": 0.99,  # Slightly reduced for more immediate rewards\n",
    "    \"GAE_LAMBDA\": 0.95,  # GAE-Lambda parameter\n",
    "    \"ENT_COEF\": 0.07,  # Increased entropy coefficient for better exploration\n",
    "    \"CLIP_EPS\": 0.1,  # PPO clip parameter\n",
    "    \"VF_COEF\": 0.5,  # Value function coefficient\n",
    "    \"MAX_GRAD_NORM\": 0.3,  # Gradient clipping\n",
    "    \"MINIBATCH_SIZE\": NUM_INNER_STEPS // NUM_UPDATES_PER_EPOCH,\n",
    "    \"NUM_UPDATES_PER_MINIBATCH\": 4,\n",
    "    \"DEVICE\": jax.devices()\n",
    "}\n",
    "\n",
    "\n",
    "trainer, current_date = make_train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd4847",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f773cc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorios base\n",
    "#local = '/mnt/lustre/home/samuloza'\n",
    "local = 'D:/OneDrive - Universidad Complutense de Madrid (UCM)/Doctorado'\n",
    "base_dirs = {\n",
    "    \"Prisioner_dilemma\": f\"{local}/data/samuel_lozano/coin_game/Prisioner_dilemma\",\n",
    "    \"No_dilemma\": f\"{local}/data/samuel_lozano/coin_game/No_dilemma\"\n",
    "}\n",
    "\n",
    "output_path = f\"{local}/data/samuel_lozano/coin_game/training_results.csv\"\n",
    "\n",
    "# Eliminar el archivo CSV si ya existe\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c7a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "\n",
    "# Patrón para capturar los coeficientes de recompensa\n",
    "reward_pattern = re.compile(r\"REWARD_COEF:\\s*\\[\\[\\s*([\\d\\.eE+-]+),\\s*([\\d\\.eE+-]+)\\],\\s*\\[\\s*([\\d\\.eE+-]+),\\s*([\\d\\.eE+-]+)\\]\\]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9910c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dilemma_name, base_dir in base_dirs.items():\n",
    "    dilemma_flag = 1 if \"Prisioner\" in dilemma_name else 0\n",
    "    for folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        date_time_str = folder.replace(\"Training_\", \"\")\n",
    "        config_path = os.path.join(folder_path, \"config.txt\")\n",
    "        csv_path = os.path.join(folder_path, \"training_stats.csv\")\n",
    "\n",
    "        if not (os.path.exists(config_path) and os.path.exists(csv_path)):\n",
    "            continue\n",
    "\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config_contents = f.read()\n",
    "        match = reward_pattern.search(config_contents)\n",
    "        if not match:\n",
    "            continue\n",
    "\n",
    "        alpha_1, beta_1, alpha_2, beta_2 = map(float, match.groups())\n",
    "\n",
    "        grid_size_match = re.search(r\"GRID_SIZE:\\s*(\\d+)\", config_contents)\n",
    "        grid_size = int(grid_size_match.group(1)) if grid_size_match else -1 \n",
    "\n",
    "        lr_match = re.search(r\"LR:\\s*([0-9.eE+-]+)\", config_contents)\n",
    "        lr = float(lr_match.group(1)) \n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        df.insert(0, \"timestamp\", date_time_str)\n",
    "        df.insert(1, \"dilemma\", dilemma_flag)\n",
    "        df.insert(2, \"alpha_1\", alpha_1)\n",
    "        df.insert(3, \"beta_1\", beta_1)\n",
    "        df.insert(4, \"alpha_2\", alpha_2)\n",
    "        df.insert(5, \"beta_2\", beta_2)\n",
    "        df.insert(6, \"grid_size\", grid_size)\n",
    "        df.insert(7, \"lr\", lr)\n",
    "\n",
    "        all_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6912adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar todos los resultados\n",
    "final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "final_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be91a57",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f0de32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el CSV especificando los tipos de datos\n",
    "dtype_dict = {\n",
    "    \"timestamp\": str,\n",
    "    \"dilemma\": int,\n",
    "    \"alpha_1\": float,\n",
    "    \"beta_1\": float,\n",
    "    \"alpha_2\": float,\n",
    "    \"beta_2\": float\n",
    "}\n",
    "\n",
    "df = pd.read_csv(output_path, dtype=dtype_dict, low_memory=False)\n",
    "for col in df.columns[6:]:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Crear una columna identificadora de combinación de coeficientes\n",
    "df = df.sort_values(by=[\"alpha_1\", \"alpha_2\"], ascending=[False, False])\n",
    "df[\"attitude_key\"] = df.apply(lambda row: f\"{row['alpha_1']}_{row['beta_1']}_{row['alpha_2']}_{row['beta_2']}\", axis=1)\n",
    "df[\"pure_reward_total\"] = df[\"pure_reward_agent_0\"] + df[\"pure_reward_agent_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "564b39bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar todas las combinaciones únicas\n",
    "unique_attitudes = df[\"attitude_key\"].unique()\n",
    "unique_lr = df[\"lr\"].unique()\n",
    "unique_dilemma = df[\"dilemma\"].unique()\n",
    "\n",
    "figures_dir = f\"{local}/data/samuel_lozano/coin_game/figures/\"\n",
    "os.makedirs(figures_dir, exist_ok=True)\n",
    "\n",
    "metrics_0 = [\n",
    "    \"own_coin_collected_agent_0\",\n",
    "    \"other_coin_collected_agent_0\",\n",
    "    \"reject_own_coin_agent_0\",\n",
    "    \"reject_other_coin_agent_0\",\n",
    "    \"no_coin_visible_agent_0\"\n",
    "]\n",
    "\n",
    "metrics_1 = [\n",
    "    \"own_coin_collected_agent_1\",\n",
    "    \"other_coin_collected_agent_1\",\n",
    "    \"reject_own_coin_agent_1\",\n",
    "    \"reject_other_coin_agent_1\",\n",
    "    \"no_coin_visible_agent_1\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61987c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Pure total reward vs epoch\n",
    "\n",
    "for attitude in unique_attitudes:\n",
    "    subset = df[df[\"attitude_key\"] == attitude]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for dilemma_value in unique_dilemma:\n",
    "        dilemma_filtered = subset[subset[\"dilemma\"] == dilemma_value]\n",
    "    \n",
    "        # Filtrar por tasa de aprendizaje\n",
    "        for lr in unique_lr:\n",
    "            lr_filtered = dilemma_filtered[dilemma_filtered[\"lr\"] == lr]\n",
    "            grouped = lr_filtered.groupby(\"epoch\")[\"pure_reward_total\"].mean().reset_index()\n",
    "            label = f\"Dilemma {dilemma_value}, LR {lr}\"\n",
    "            plt.plot(grouped[\"epoch\"], grouped[\"pure_reward_total\"], label=label)\n",
    "    \n",
    "    # Añadir detalles\n",
    "    plt.title(f\"Pure Reward vs Epoch\\nAttitude {attitude}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    #plt.xlim([0,100])\n",
    "    plt.ylabel(\"Pure Reward Total\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    sanitized_attitude = attitude.replace('.', 'p')\n",
    "    filename = f\"pure_reward_attitude_{sanitized_attitude}.png\"\n",
    "    filepath = os.path.join(figures_dir, filename)\n",
    "    plt.savefig(filepath)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cab83c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Each agent pure total reward vs epoch\n",
    "\n",
    "for attitude in unique_attitudes:\n",
    "    subset = df[df[\"attitude_key\"] == attitude]\n",
    "\n",
    "    for dilemma_value in unique_dilemma:\n",
    "        dilemma_filtered = subset[subset[\"dilemma\"] == dilemma_value]\n",
    "    \n",
    "        # Crear la figura\n",
    "        plt.figure(figsize=(10, 6))\n",
    "    \n",
    "        # Filtrar por tasa de aprendizaje\n",
    "        for lr in unique_lr:\n",
    "            lr_filtered = dilemma_filtered[dilemma_filtered[\"lr\"] == lr]\n",
    "            grouped = lr_filtered.groupby(\"epoch\")[[\"pure_reward_agent_0\", \"pure_reward_agent_1\"]].mean().reset_index()\n",
    "            label_0 = f\"Agent 0, LR {lr}\"\n",
    "            label_1 = f\"Agent 1, LR {lr}\"\n",
    "            plt.plot(grouped[\"epoch\"], grouped[\"pure_reward_agent_0\"], label=label_0)\n",
    "            plt.plot(grouped[\"epoch\"], grouped[\"pure_reward_agent_1\"], label=label_1)\n",
    "    \n",
    "        # Añadir detalles\n",
    "        plt.title(f\"Pure Reward vs Epoch\\nAttitude {attitude}, Dilemma {dilemma_value}\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        #plt.xlim([0,100])\n",
    "        plt.ylabel(\"Pure Reward Total\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        sanitized_attitude = attitude.replace('.', 'p')\n",
    "        filename = f\"pure_reward_agents_d{dilemma_value}_attitude_{sanitized_attitude}.png\"\n",
    "        filepath = os.path.join(figures_dir, filename)\n",
    "        plt.savefig(filepath)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e3797d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print agent metrics vs epoch\n",
    "for attitude in unique_attitudes:\n",
    "    subset = df[df[\"attitude_key\"] == attitude]\n",
    "\n",
    "    att_parts = attitude.split('_')\n",
    "    att0_title = f\"{att_parts[0]}_{att_parts[1]}\"\n",
    "    att1_title = f\"{att_parts[2]}_{att_parts[3]}\"\n",
    "\n",
    "    for dilemma_value in [0, 1]:\n",
    "        for grid_size in subset[\"grid_size\"].unique():\n",
    "            for lr in subset[\"lr\"].unique():\n",
    "                filtered_subset = subset[(subset[\"dilemma\"] == dilemma_value) & (subset[\"grid_size\"] == grid_size) & (subset[\"lr\"] == lr)]\n",
    "    \n",
    "                plt.figure(figsize=(12, 6))\n",
    "                for metric in metrics_0:\n",
    "                    grouped = filtered_subset.groupby([\"epoch\"])[metric].mean().reset_index()\n",
    "                    plt.plot(grouped[\"epoch\"], grouped[metric], label=metric.replace(\"_\", \" \").title())\n",
    "                plt.title(f\"Metrics per Epoch - Dilemma {dilemma_value}, LR {lr}, Attitude {att0_title}\")\n",
    "                plt.xlabel(\"Epoch\")\n",
    "                plt.ylabel(\"Mean value\")\n",
    "                plt.legend()\n",
    "                #plt.xlim([0, 100])\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                sanitized_attitude = attitude.replace('.', 'p')\n",
    "                filename_0 = f\"metrics_agent0_d{dilemma_value}_lr{str(lr).replace('.', 'p')}_attitude_{sanitized_attitude}.png\"\n",
    "                filepath_0 = os.path.join(figures_dir, filename_0)\n",
    "                plt.savefig(filepath_0)\n",
    "                plt.close()\n",
    "    \n",
    "                plt.figure(figsize=(12, 6))\n",
    "                for metric in metrics_1:\n",
    "                    grouped = filtered_subset.groupby([\"epoch\"])[metric].mean().reset_index()\n",
    "                    plt.plot(grouped[\"epoch\"], grouped[metric], label=metric.replace(\"_\", \" \").title())\n",
    "                plt.title(f\"Metrics per Epoch - Dilemma {dilemma_value}, LR {lr}, Attitude {att1_title}\")\n",
    "                plt.xlabel(\"Epoch\")\n",
    "                plt.ylabel(\"Mean value\")\n",
    "                plt.legend()\n",
    "                #plt.xlim([0, 100])\n",
    "                plt.tight_layout()\n",
    "\n",
    "                filename_1 = f\"metrics_agent1_d{dilemma_value}_lr{str(lr).replace('.', 'p')}_attitude_{sanitized_attitude}.png\"\n",
    "                filepath_1 = os.path.join(figures_dir, filename_1)\n",
    "                plt.savefig(filepath_1)\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee4b108",
   "metadata": {},
   "source": [
    "## Averaging over attitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ac06838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Pure total reward vs epoch\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for dilemma_value in unique_dilemma:\n",
    "    dilemma_filtered_df = df[df[\"dilemma\"] == dilemma_value]\n",
    "\n",
    "    # Filtrar por tasa de aprendizaje\n",
    "    for lr in unique_lr:\n",
    "        lr_filtered_df = dilemma_filtered_df[dilemma_filtered_df[\"lr\"] == lr]\n",
    "        grouped = lr_filtered_df.groupby(\"epoch\")[\"pure_reward_total\"].mean().reset_index()\n",
    "        label = f\"Dilemma {dilemma_value}, LR {lr}\"\n",
    "        plt.plot(grouped[\"epoch\"], grouped[\"pure_reward_total\"], label=label)\n",
    "\n",
    "# Añadir detalles\n",
    "plt.title(f\"Pure Reward vs Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "#plt.xlim([0,100])\n",
    "plt.ylabel(\"Pure Reward Total\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "filename = f\"pure_reward.png\"\n",
    "filepath = os.path.join(figures_dir, filename)\n",
    "plt.savefig(filepath)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea97ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Each agent pure total reward vs epoch\n",
    "\n",
    "for dilemma_value in unique_dilemma:\n",
    "    dilemma_filtered = subset[subset[\"dilemma\"] == dilemma_value]\n",
    "\n",
    "    # Crear la figura\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Filtrar por tasa de aprendizaje\n",
    "    for lr in unique_lr:\n",
    "        lr_filtered = dilemma_filtered[dilemma_filtered[\"lr\"] == lr]\n",
    "        grouped = lr_filtered.groupby(\"epoch\")[[\"pure_reward_agent_0\", \"pure_reward_agent_1\"]].mean().reset_index()\n",
    "        label_0 = f\"Agent 0, LR {lr}\"\n",
    "        label_1 = f\"Agent 1, LR {lr}\"\n",
    "        plt.plot(grouped[\"epoch\"], grouped[\"pure_reward_agent_0\"], label=label_0)\n",
    "        plt.plot(grouped[\"epoch\"], grouped[\"pure_reward_agent_1\"], label=label_1)\n",
    "\n",
    "    # Añadir detalles\n",
    "    plt.title(f\"Pure Reward vs Epoch\\nDilemma {dilemma_value}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    #plt.xlim([0,100])\n",
    "    plt.ylabel(\"Pure Reward Total\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    sanitized_attitude = attitude.replace('.', 'p')\n",
    "    filename = f\"pure_reward_agents_d{dilemma_value}.png\"\n",
    "    filepath = os.path.join(figures_dir, filename)\n",
    "    plt.savefig(filepath)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52586e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print agent metrics vs epoch\n",
    "\n",
    "for dilemma_value in unique_dilemma:\n",
    "    for grid_size in df[\"grid_size\"].unique():\n",
    "        for lr in unique_lr:\n",
    "            filtered_df = df[(df[\"dilemma\"] == dilemma_value) & (df[\"grid_size\"] == grid_size) & (df[\"lr\"] == lr)]\n",
    "\n",
    "            plt.figure(figsize=(12, 6))\n",
    "\n",
    "            for metric in metrics_0:\n",
    "                grouped = filtered_df.groupby([\"epoch\"])[metric].mean().reset_index()\n",
    "                plt.plot(grouped[\"epoch\"], grouped[metric], label=metric.replace(\"_\", \" \").title())\n",
    "\n",
    "            plt.title(f\"Metrics per Epoch - Dilemma {dilemma_value}, LR {lr}\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Mean value\")\n",
    "            plt.legend()\n",
    "            #plt.xlim([0, 100])\n",
    "            plt.tight_layout()\n",
    "            filename_0 = f\"metrics_agent0_d{dilemma_value}_lr{str(lr).replace('.', 'p')}.png\"\n",
    "            filepath_0 = os.path.join(figures_dir, filename_0)\n",
    "            plt.savefig(filepath_0)\n",
    "            plt.close()\n",
    "\n",
    "            plt.figure(figsize=(12, 6))\n",
    "\n",
    "            for metric in metrics_1:\n",
    "                grouped = filtered_df.groupby([\"epoch\"])[metric].mean().reset_index()\n",
    "                plt.plot(grouped[\"epoch\"], grouped[metric], label=metric.replace(\"_\", \" \").title())\n",
    "\n",
    "            plt.title(f\"Metrics per Epoch - Dilemma {dilemma_value}, LR {lr}\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Mean value\")\n",
    "            plt.legend()\n",
    "            #plt.xlim([0, 100])\n",
    "            plt.tight_layout()\n",
    "            filename_1 = f\"metrics_agent1_d{dilemma_value}_lr{str(lr).replace('.', 'p')}.png\"\n",
    "            filepath_1 = os.path.join(figures_dir, filename_1)\n",
    "            plt.savefig(filepath_1)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa214b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JaxMARL_TFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
